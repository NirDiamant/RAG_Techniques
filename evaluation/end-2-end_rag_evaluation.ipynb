{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "This hands-on tutorial walks participants through building an automated evaluation pipeline for RAG applications. Using real examples, we’ll define key evaluation criteria and implement simple methods to assess LLM output quality—focusing on completeness, relevance, and hallucinations. Presented at DataNights Course.\n",
    "\n",
    "**This tutorial will cover:**\n",
    "\n",
    "1. How to choose key evaluation criteria for your use case\n",
    "\n",
    "2. Selecting the right data and KPIs for metric evaluation\n",
    "\n",
    "3. Building an LLM-as-a-judge metric for a chosen criterion\n",
    "\n",
    "4. Using open-source metrics like RAGAS\n",
    "\n",
    "5. Aggregating metrics into an end-to-end evaluation pipeline\n",
    "\n",
    "\n",
    "In this tutorial we will use a modifed version of the [RAG-12000 dataset](https://huggingface.co/datasets/neural-bridge/rag-dataset-12000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import Union, List, Optional, Type\n",
    "from openai import AsyncAzureOpenAI\n",
    "import asyncio\n",
    "import random\n",
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "dotenv.load_dotenv('/Users/nadav/Desktop/GitRepos/llm/.env')\n",
    "\n",
    "# Configuration – use environment variables or directly set values\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"https://your-resource-name.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") or \"your-azure-api-key\"\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\") or \"gpt-4o-mini\"\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Create an Azure OpenAI client\n",
    "client = AsyncAzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "\n",
    "async def run_llm_call(system_prompt: str, user_prompt: str,\n",
    "                       response_model: Optional[Type[BaseModel]] = None,\n",
    "                       model: str = AZURE_DEPLOYMENT_NAME) -> Union[str, None]:\n",
    "    max_retries = 5\n",
    "    base_delay = 30  # seconds\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if response_model:\n",
    "                response = await client.beta.chat.completions.parse(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    response_format=response_model\n",
    "                )\n",
    "                return response.choices[0].message.parsed\n",
    "            else:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} - Azure OpenAI Error: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            # Random sleep between retries\n",
    "            sleep_time = base_delay * (2 ** (attempt)) + random.uniform(0, 1)\n",
    "            await asyncio.sleep(sleep_time)\n",
    "\n",
    "    \n",
    "\n",
    "async def run_llm_calls(system_prompts: List[str], user_prompts: List[str], \n",
    "                        response_model: Optional[Type[BaseModel]] = None,\n",
    "                        model: str = AZURE_DEPLOYMENT_NAME) -> List[str]:\n",
    "    tasks = [\n",
    "        run_llm_call(system_prompt, user_prompt, model=model, \n",
    "                     response_model=response_model)\n",
    "        for system_prompt, user_prompt in zip(system_prompts, user_prompts)\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Choosing Evaluation Criteria\n",
    "\n",
    "Define what “good” means for your use case. We'll identify key dimensions like relevance, accuracy, fluency, and helpfulness, based on your product goals provided via feedback of human annotators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>information_retrieved</th>\n",
       "      <th>output</th>\n",
       "      <th>annotation</th>\n",
       "      <th>annotation_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the original release of Johnny Turbo’...</td>\n",
       "      <td>Johnny Turbo’s Arcade: Express Raider Date And...</td>\n",
       "      <td>Johnny Turbo’s Arcade: Express Raider was orig...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the CEO of Franklin Templeton Investments?</td>\n",
       "      <td>Gregory Johnson\\nCEO\\nUpdated On : Sep 28, 201...</td>\n",
       "      <td>The CEO of Franklin Templeton Investments is G...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  When was the original release of Johnny Turbo’...   \n",
       "1  Who is the CEO of Franklin Templeton Investments?   \n",
       "\n",
       "                               information_retrieved  \\\n",
       "0  Johnny Turbo’s Arcade: Express Raider Date And...   \n",
       "1  Gregory Johnson\\nCEO\\nUpdated On : Sep 28, 201...   \n",
       "\n",
       "                                              output annotation  \\\n",
       "0  Johnny Turbo’s Arcade: Express Raider was orig...       good   \n",
       "1  The CEO of Franklin Templeton Investments is G...       good   \n",
       "\n",
       "  annotation_reasoning  \n",
       "0                  NaN  \n",
       "1                  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://figshare.com/ndownloader/files/53919875')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"While the response correctly states the main statement of the Equal Rights Amendment, it includes a lot of additional information that is not directly relevant to the customer's question. This verbosity can overwhelm the customer and distract from the clear, concise answer they are seeking. Including details about the history, ratification process, and opposition arguments, although related to the ERA, goes beyond the scope of the question and may confuse or frustrate customers looking for a straightforward explanation. As a customer support manager, I would advise keeping responses focused and succinct to maintain clarity and customer satisfaction.\",\n",
       " \"While the response correctly defines the theory of Panspermia, it omits the important detail of who first proposed it, which is essential to fully answer the customer's question. This missing information could lead to customer dissatisfaction as their query about the originator of the theory remains unanswered. It's important to provide complete information to maintain trust and clarity in our support communications.\",\n",
       " \"While the response correctly identifies metacognition as the main focus of the professional development work with Mike Anderson, it is overly verbose and includes many details that, although related to the broader context, are not directly supported by the specific information provided. This verbosity can distract from the core message and may confuse readers who are looking for a concise and precise answer. As a customer support manager, I would advise the QA team to watch for responses that, while appearing thorough, include unnecessary elaborations that do not directly address the customer's question or the provided context. Such responses can reduce clarity and efficiency in communication, which are critical in customer support interactions.\",\n",
       " \"This response provides the original release year and mentions the pre-order availability on the Nintendo Switch, which seems helpful at first glance. However, it omits the specific launch date of July 12th, 2018, which is crucial for customers wanting to know exactly when the game will be available. Leaving out the launch date can cause confusion and may lead to customers missing the release or pre-order window, so it's important to include that detail in the response.\",\n",
       " 'This response appears plausible because Brian Westbrook is a well-known player for the Eagles and could be expected to score touchdowns. However, it contradicts the provided context, which clearly states that Donovan McNabb scored the first touchdown on a QB sneak. This subtle change might be overlooked at first glance but would be caught upon closer review, making it a useful test for attention to detail.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will start by manually reviewing some of the manual annotation reasons\n",
    "# To get a sense of the use case and its potential issues\n",
    "\n",
    "df[df['annotation'] == 'bad']['annotation_reasoning'].sample(5).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user feedback provided, here are five common problems identified in the agent responses:\n",
      "\n",
      "1. **Introduction of Unsupported Details**: Many responses include additional information or suggestions that are not supported by the original context. This can mislead customers and create confusion, as seen in examples where recommendations or details about events, practices, or features were added without basis.\n",
      "\n",
      "2. **Verbosity and Lack of Conciseness**: Several responses are overly verbose, including excessive details that distract from the main question. This can overwhelm customers and obscure the key information they are seeking, making it difficult for them to quickly grasp the essential points.\n",
      "\n",
      "3. **Inaccurate Representation of Context**: Some responses contradict or misinterpret the provided context, leading to inaccuracies in the information conveyed. This includes misrepresenting facts, such as incorrectly stating the roles or opinions of individuals or organizations, which can undermine trust in the responses.\n",
      "\n",
      "4. **Omission of Critical Information**: Important details that are necessary for a complete understanding of the topic are often omitted. This can result in incomplete answers that fail to fully address the customer's inquiry, leading to potential dissatisfaction.\n",
      "\n",
      "5. **Misleading Information**: Certain responses contain inaccuracies that could mislead customers about products, services, or concepts. This includes incorrect claims about features, functionalities, or historical facts that are not aligned with the provided context, which can lead to confusion and mistrust.\n",
      "\n",
      "Addressing these issues would enhance the clarity, accuracy, and overall effectiveness of the agent responses.\n"
     ]
    }
   ],
   "source": [
    "# We will use an llm to analyze the feedback and summarize the common problems\n",
    "# in the agent responses.\n",
    "\n",
    "sys_message_analyze_reasons = \"\"\"\n",
    "You are an amazing data analyst analyzing user feedback provided for a question answering agent responses.\n",
    "Your task is to analyze the user feedback and summarize the what are the common problemsin the agent responses.\n",
    "return up to 5 common problems.\n",
    "\"\"\".strip()\n",
    "\n",
    "only_bad = df[df['annotation'] == 'bad'].copy()\n",
    "responses = await run_llm_calls(\n",
    "    system_prompts=[sys_message_analyze_reasons],\n",
    "    user_prompts=['\\n'.join(only_bad['annotation_reasoning'].tolist())],\n",
    ")\n",
    "\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, the errors can be grouped into four categories:\n",
    "\n",
    "1. Incomplete – Missing key information or context\n",
    "\n",
    "2. Inconscise – Unnecessarily wordy or repetitive responses\n",
    "\n",
    "3. Hallucinations – Fabricated or factually incorrect content\n",
    "\n",
    "4. Contradictions – Statements that conflict with the source or other parts of the response\n",
    "\n",
    "\n",
    "### Divide and Conquer\n",
    "Although it's technically possible to evaluate all four criteria in a single LLM-as-a-judge call, recent studies—and the collective experience of many practitioners—show that RAG evaluation is complex. Breaking the task into smaller, focused components usally delivers better accuracy and insight.\n",
    "\n",
    "We’ll start by implementing an LLM-as-a-judge method to evaluate *completeness*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 – Building the Benchmark for Completeness\n",
    "Why does this deserve its own section? Because the naive approach is tempting—but wrong.\n",
    "\n",
    "Not every sample labeled as “bad” is bad due to low completeness. If we don’t filter carefully, we risk evaluating against the wrong signals. To build a meaningful benchmark, we need to isolate true negatives—cases that are specifically incomplete, not flawed for other reasons like hallucinations or contradictions.\n",
    "\n",
    "And since we’re not fans of manual work, we’ll use our BFF ChatGPT to help automate the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "completeness_related\n",
       "False    28\n",
       "True     12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt_is_complete = \"\"\"\n",
    "You are evaluating whether a response from a question-answering agent is incomplete.\n",
    "\n",
    "Your task: Based on the user feedback, determine if the primary issue with the agent’s answer is a **completeness problem**—i.e., it is missing key information that should have been included. If the issue is due to something else (e.g., hallucination, contradiction, poor phrasing), mark it as not related to completeness.\n",
    "\n",
    "Respond with a clear yes/no judgment and a brief reasoning.\n",
    "\"\"\".strip()\n",
    "\n",
    "user_prompt_reasoning_eval = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Wrong Answer:\n",
    "{output}\n",
    "\n",
    "Annotation Reasoning:\n",
    "{reasoning}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class CompletenessRelated(BaseModel):\n",
    "    reasoning: str\n",
    "    has_completeness_problem: bool\n",
    "\n",
    "\n",
    "sys_msgs = [system_prompt_is_complete] * df.shape[0]\n",
    "user_msgs = [\n",
    "    user_prompt_reasoning_eval.format(\n",
    "        question=row['input'],\n",
    "        output=row['output'],\n",
    "        reasoning=row['annotation_reasoning'],\n",
    "    )\n",
    "    for _, row in only_bad.iterrows()\n",
    "]\n",
    "responses = await run_llm_calls(sys_msgs, user_msgs, response_model=CompletenessRelated)\n",
    "only_bad['completeness_related'] = [r.has_completeness_problem for r in responses]\n",
    "\n",
    "only_bad['completeness_related'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/011yh4l51n51hzf8z_54kzrm0000gn/T/ipykernel_86202/4090687364.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  completeness_eval_df['annotation'] = completeness_eval_df['annotation'].replace({'good': 1, 'bad': 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>information_retrieved</th>\n",
       "      <th>output</th>\n",
       "      <th>annotation</th>\n",
       "      <th>annotation_reasoning</th>\n",
       "      <th>completeness_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who won the gold medal in the men's 1,500m fin...</td>\n",
       "      <td>+50 points in the past 30 days\\nNing is the so...</td>\n",
       "      <td>China's Ning Zhongyan won the gold medal in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What are some of the challenges Amy Bloom face...</td>\n",
       "      <td>Steven G. Smith for The Boston Globe\\nAuthor A...</td>\n",
       "      <td>Amy Bloom finds getting started on a significa...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "2   Who won the gold medal in the men's 1,500m fin...   \n",
       "62  What are some of the challenges Amy Bloom face...   \n",
       "\n",
       "                                information_retrieved  \\\n",
       "2   +50 points in the past 30 days\\nNing is the so...   \n",
       "62  Steven G. Smith for The Boston Globe\\nAuthor A...   \n",
       "\n",
       "                                               output  annotation  \\\n",
       "2   China's Ning Zhongyan won the gold medal in th...           1   \n",
       "62  Amy Bloom finds getting started on a significa...           1   \n",
       "\n",
       "   annotation_reasoning completeness_related  \n",
       "2                   NaN                  NaN  \n",
       "62                  NaN                  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the sake of getting a sample which is representative of the data but not overly unbalanced\n",
    "# we will use 20 examples of the positive class and the 10 examples of the negative class.\n",
    "\n",
    "only_good = df[df['annotation'] == 'good'].copy()\n",
    "completeness_eval_df = pd.concat([only_good.sample(20, random_state=1), only_bad[only_bad['completeness_related']]])\n",
    "completeness_eval_df['annotation'] = completeness_eval_df['annotation'].replace({'good': 1, 'bad': 0})\n",
    "\n",
    "print(completeness_eval_df.shape)\n",
    "completeness_eval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 – Completeness Metric\n",
    "\n",
    "We'll use an LLM to assess whether each output is complete. First, we'll implement a simple baseline approach—then iterate to improve accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_method\n",
      "1    32\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 12.0},\n",
       " '1': {'precision': 0.625,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.7692307692307693,\n",
       "  'support': 20.0},\n",
       " 'accuracy': 0.625,\n",
       " 'macro avg': {'precision': 0.3125,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.38461538461538464,\n",
       "  'support': 32.0},\n",
       " 'weighted avg': {'precision': 0.390625,\n",
       "  'recall': 0.625,\n",
       "  'f1-score': 0.4807692307692308,\n",
       "  'support': 32.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "system_prompt_completeness_eval = \"\"\"\n",
    "Decide if the answer is missing important information based on the question.\n",
    "\n",
    "Is the answer complete? Answer complete or incomplete.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "user_prompt_answer_eval = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{conclusions}\n",
    "\"\"\".strip()\n",
    "\n",
    "class ScoreValue(int, Enum):\n",
    "    INCOMPLETE = 0\n",
    "    COMPLETE = 1\n",
    "\n",
    "class CompletenessScore(BaseModel):\n",
    "    score: ScoreValue\n",
    "\n",
    "sys_msgs = [system_prompt_completeness_eval] * completeness_eval_df.shape[0]\n",
    "user_msgs = [\n",
    "    user_prompt_answer_eval.format(\n",
    "        question=row['input'],\n",
    "        context=row['information_retrieved'],\n",
    "        conclusions=row['output']\n",
    "    )\n",
    "    for _, row in completeness_eval_df.iterrows()\n",
    "]\n",
    "responses = await run_llm_calls(sys_msgs, user_msgs, response_model=CompletenessScore)\n",
    "completeness_eval_df['naive_method'] = [int(x.score.value) for x in responses]\n",
    "\n",
    "print(completeness_eval_df.naive_method.value_counts())\n",
    "classification_report(\n",
    "    completeness_eval_df.annotation,\n",
    "    completeness_eval_df.naive_method,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive method struggles to identify *incomplete negative examples* accurately. This shows the problem is more complex than it seems—and not easily solved with a simple approach. Let’s explore more advanced methods to handle it more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advenced_method\n",
      "1    18\n",
      "0    14\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.7857142857142857,\n",
       "  'recall': 0.9166666666666666,\n",
       "  'f1-score': 0.8461538461538461,\n",
       "  'support': 12.0},\n",
       " '1': {'precision': 0.9444444444444444,\n",
       "  'recall': 0.85,\n",
       "  'f1-score': 0.8947368421052632,\n",
       "  'support': 20.0},\n",
       " 'accuracy': 0.875,\n",
       " 'macro avg': {'precision': 0.8650793650793651,\n",
       "  'recall': 0.8833333333333333,\n",
       "  'f1-score': 0.8704453441295547,\n",
       "  'support': 32.0},\n",
       " 'weighted avg': {'precision': 0.8849206349206349,\n",
       "  'recall': 0.875,\n",
       "  'f1-score': 0.8765182186234818,\n",
       "  'support': 32.0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt_completeness_eval = \"\"\"\n",
    "You are an assistant evaluating how complete an answer is, given a question and supporting context.\n",
    "\n",
    "First, consider what are the key pieces of information a fully complete answer should include based on the question and context.  \n",
    "Then, check whether the answer contains all of that information.\n",
    "\n",
    "Provide a short explanation of your reasoning.  \n",
    "Then assign a score:\n",
    "\n",
    "0 - Not complete: Key information is missing or major parts of the question are not addressed.  \n",
    "1 - Partially complete: All parts of the question are touched on, but some are incomplete or only loosely supported by the context.  \n",
    "2 - Fully complete: The answer thoroughly and directly addresses all aspects of the question, using information clearly supported by the context.\n",
    "\"\"\".strip()\n",
    "\n",
    "class ScoreValue(int, Enum):\n",
    "    INCOMPLETE = 0\n",
    "    PARTIALLY_COMPLETE = 1\n",
    "    COMPLETE = 2\n",
    "\n",
    "class CompletenessScore(BaseModel):\n",
    "    reasoning: str\n",
    "    score: ScoreValue\n",
    "\n",
    "\n",
    "sys_msgs = [system_prompt_completeness_eval] * completeness_eval_df.shape[0]\n",
    "responses = await run_llm_calls(sys_msgs, user_msgs, response_model=CompletenessScore, model=\"gpt-4.1-mini\")\n",
    "completeness_eval_df['advenced_method'] = [int(x.score > 1) for x in responses]\n",
    "completeness_eval_df['advenced_method_reasoning'] = [x.reasoning for x in responses]\n",
    "\n",
    "print(completeness_eval_df.advenced_method.value_counts())\n",
    "classification_report(\n",
    "    completeness_eval_df.annotation,\n",
    "    completeness_eval_df.advenced_method,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advanced method performs well and reliably identifies incomplete answers. While there’s still some room for fine-tuning, it’s accurate enough for practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Utilaze RAGAS for hallucination detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factual_related\n",
      "True     22\n",
      "False    18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First we will create and evalution set in a similar approach to the one we used for the completeness evaluation\n",
    "\n",
    "system_prompt_is_factually_correct = \"\"\"\n",
    "You are evaluating whether a response from a question-answering agent has a **factual correctness problem**.\n",
    "\n",
    "Your task: Based on the user feedback, determine if the main issue with the answer is related to **hallucination** (made-up or incorrect information) or **contradiction** (statements that conflict with known facts or the context). If the issue is something else (e.g., incomplete, vague, poorly phrased), mark it as not related to factual correctness.\n",
    "\n",
    "Respond with a clear yes/no judgment and a brief explanation.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class FactualRelated(BaseModel):\n",
    "    reasoning: str\n",
    "    has_factual_correctness_problem: bool\n",
    "\n",
    "\n",
    "sys_msgs = [system_prompt_is_factually_correct] * df.shape[0]\n",
    "user_msgs = [\n",
    "    user_prompt_reasoning_eval.format(\n",
    "        question=row['input'],\n",
    "        output=row['output'],\n",
    "        reasoning=row['annotation_reasoning'],\n",
    "    )\n",
    "    for _, row in only_bad.iterrows()\n",
    "]\n",
    "responses = await run_llm_calls(sys_msgs, user_msgs, response_model=FactualRelated)\n",
    "only_bad['factual_related'] = [r.has_factual_correctness_problem for r in responses]\n",
    "print(only_bad['factual_related'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/011yh4l51n51hzf8z_54kzrm0000gn/T/ipykernel_86202/4209158597.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  factual_eval_df['annotation'] = factual_eval_df['annotation'].replace({'good': 1, 'bad': 0})\n"
     ]
    }
   ],
   "source": [
    "factual_eval_df = pd.concat([only_good.sample(sum(only_bad['factual_related']), random_state=1), \n",
    "                             only_bad[only_bad['factual_related']]])\n",
    "factual_eval_df['annotation'] = factual_eval_df['annotation'].replace({'good': 1, 'bad': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b125cd5a5a9841aa8552eb495077c3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas_faithfulness_binary\n",
      "1    36\n",
      "0     8\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.875,\n",
       "  'recall': 0.3181818181818182,\n",
       "  'f1-score': 0.4666666666666667,\n",
       "  'support': 22.0},\n",
       " '1': {'precision': 0.5833333333333334,\n",
       "  'recall': 0.9545454545454546,\n",
       "  'f1-score': 0.7241379310344828,\n",
       "  'support': 22.0},\n",
       " 'accuracy': 0.6363636363636364,\n",
       " 'macro avg': {'precision': 0.7291666666666667,\n",
       "  'recall': 0.6363636363636364,\n",
       "  'f1-score': 0.5954022988505747,\n",
       "  'support': 44.0},\n",
       " 'weighted avg': {'precision': 0.7291666666666667,\n",
       "  'recall': 0.6363636363636364,\n",
       "  'f1-score': 0.5954022988505747,\n",
       "  'support': 44.0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test RAGAS\n",
    "\n",
    "import os\n",
    "from ragas.metrics import faithfulness\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "\n",
    "langchain_client = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "samples = [SingleTurnSample(\n",
    "    user_input=row['input'],\n",
    "    response=row['output'],\n",
    "    retrieved_contexts=[row['information_retrieved']],\n",
    ") for _, row in factual_eval_df.iterrows()]\n",
    "\n",
    "results = evaluate(EvaluationDataset(samples=samples), metrics=[faithfulness], llm=langchain_client)\n",
    "factual_eval_df['ragas_faithfulness'] = [r['faithfulness'] for r in results.scores]\n",
    "factual_eval_df['ragas_faithfulness_binary'] = [1 if r['faithfulness'] > 0.5 else 0 for r in results.scores]\n",
    "print(factual_eval_df.ragas_faithfulness_binary.value_counts())\n",
    "\n",
    "classification_report(\n",
    "    factual_eval_df.annotation,\n",
    "    factual_eval_df.ragas_faithfulness_binary,\n",
    "    output_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, even the most advanced open-source tools often fall short on complex tasks. While they perform well in general settings, they don’t meet the specific needs of our use case. Addressing this gap will require targeted research to develop a factual correctness tool tailored to our domain. One example of a non-LLM-based approach can be found [here](https://arxiv.org/abs/2504.15771)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Aggragate to an end-2-end evaluation pipeline\n",
    "\n",
    "At this stage, we combine all evaluation steps into a full end-to-end pipeline. Each sample—input, context, and LLM output—is judged on whether the output is good enough, from a domain expert’s perspective, to be sent to a client. With a representative test set, this pipeline helps measure the real-world performance of the application and track what actual application quality.\n",
    "\n",
    "When aggregating evaluation results, simply averaging scores across criteria can be misleading—some dimensions may mask critical weaknesses in others. Instead of collapsing everything into a single number, it’s often better to treat each criterion independently. The approach we will take here is to define pass/fail thresholds per criterion and only consider an output successful if it meets all of them. Another option is to use logical rules (e.g., must pass factual accuracy and clarity, but fluency can be slightly relaxed) or weighted thresholds tailored to the use case. The key is to reflect real-world standards—especially when outputs are client-facing and failure in one area can undermine the whole result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this example we will only take into account evaluation of completeness and factual correctness\n",
    "# We will use the advanced method for completeness and the RAGAS for factual correctness\n",
    "\n",
    "def get_final_annotation(row):\n",
    "    factual_score = evalaute_factuality_via_ragas(row)\n",
    "    completeness_score = evalaute_completeness_via_llm_as_judge(row)\n",
    "\n",
    "    if factual_score < FACTUALITY_THRESHOLD:\n",
    "        return \"bad\", \"factuality problem\"\n",
    "    elif completeness_score < COMPLETENESS_THRESHOLD:\n",
    "        return \"bad\", \"completeness problem\"\n",
    "    return \"good\", \"\"\n",
    "\n",
    "\n",
    "df['automated_annotation_pipeline'] = df.apply(get_final_annotation, axis=1)\n",
    "classification_report(\n",
    "    df.annotation,\n",
    "    df.automated_annotation_pipeline,\n",
    "    output_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "You’ve now got a working evaluation pipeline for RAG that combines open-source tools with custom LLM-based judgment. The goal here isn’t perfection—it’s iteration. Use this framework as a foundation. Tweak the metrics, expand the dataset, adjust for your domain. The key takeaway is that RAG systems need feedback loops. Without evaluation, you're guessing.\n",
    "\n",
    "While this notebook focused on RAG, the same concepts apply to any LLM-based application—summarization, question answering, agentic workflows, and beyond. Evaluation isn't just a final step; it’s part of the development cycle. Build it in early, keep it lightweight, and adapt as your system evolves.\n",
    "\n",
    "## About Me\n",
    "\n",
    "[Nadav Barak](https://www.linkedin.com/in/nadavbarak/) is a Head of AI at [Deepchecks](https://www.deepchecks.com/), a startup building tools to evaluate and monitor Generative AI systems. His work focuses on making LLM-based applications more reliable, measurable, and production-ready bridging the gap between cutting-edge research and real-world use.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_other",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
