{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9dfee9",
   "metadata": {},
   "source": [
    "# üîó Local Graph RAG with Verifiable Attribution\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Graph RAG** (Retrieval-Augmented Generation using Knowledge Graphs) with a focus on:\n",
    "\n",
    "1. **üîí Privacy-First**: Runs entirely locally using Ollama (no data leaves your machine)\n",
    "2. **üîó Multi-Hop Reasoning**: Connects disjoint facts that Vector RAG cannot\n",
    "3. **üìö Verifiable Attribution**: Every claim traces back to exact source sentences\n",
    "\n",
    "## Why Graph RAG?\n",
    "\n",
    "| Challenge | Vector RAG | Graph RAG |\n",
    "|-----------|------------|-----------|\n",
    "| \"Who mentored the person who founded Company X?\" | ‚ùå Struggles (requires connecting 2+ facts) | ‚úÖ Traverses relationships |\n",
    "| \"How are these two concepts related?\" | ‚ùå Finds similar chunks, not connections | ‚úÖ Follows explicit edges |\n",
    "| \"Which sentence generated this claim?\" | ‚ùå Approximate chunk attribution | ‚úÖ Exact source mapping |\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  INDEXING PIPELINE                                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Documents‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ LLM Extraction ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Knowledge Graph      ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ          ‚îÇ   ‚îÇ (Entities+Rels)‚îÇ   ‚îÇ (Neo4j/NetworkX)     ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  QUERY PIPELINE                                                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Query ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇVector Search‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇGraph Travers‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ LLM +     ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ       ‚îÇ   ‚îÇ(Entry Point)‚îÇ   ‚îÇ(Multi-hop) ‚îÇ   ‚îÇ Citations ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Key Differentiator: Verifiable Attribution\n",
    "\n",
    "Unlike traditional RAG that returns \"chunks,\" Graph RAG with attribution returns:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"answer\": \"Dr. Smith developed the XYZ algorithm...\",\n",
    "  \"citations\": [\n",
    "    {\n",
    "      \"claim\": \"Dr. Smith developed the XYZ algorithm\",\n",
    "      \"source_document\": \"research_paper.pdf\",\n",
    "      \"source_sentence\": \"Dr. Jane Smith developed the XYZ algorithm in 2022.\",\n",
    "      \"graph_path\": [\"Dr. Smith\", \"DEVELOPED\", \"XYZ Algorithm\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0ce82",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "We'll use:\n",
    "- **Ollama**: Local LLM inference (Llama 3.1)\n",
    "- **Neo4j**: Graph database (or NetworkX for lightweight demo)\n",
    "- **sentence-transformers**: For vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ollama networkx numpy sentence-transformers\n",
    "\n",
    "# For production use with Neo4j (optional):\n",
    "# !pip install neo4j\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad60c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Ollama not available. Install with: pip install ollama\")\n",
    "    print(\"   Also ensure Ollama is running: ollama serve\")\n",
    "\n",
    "print(f\"‚úÖ Ollama available: {OLLAMA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecc5ae",
   "metadata": {},
   "source": [
    "## 2. Configure Local LLM with Ollama\n",
    "\n",
    "Ollama provides local LLM inference, ensuring **complete data privacy**.\n",
    "\n",
    "### Prerequisites\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Pull a model: `ollama pull llama3.1`\n",
    "3. Ensure Ollama is running: `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a967b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"llama3.1\"  # or \"mistral\", \"phi3\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Local sentence transformer\n",
    "\n",
    "# Initialize embedding model (runs locally)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"‚úÖ Embedding model loaded: {EMBEDDING_MODEL}\")\n",
    "\n",
    "def call_llm(prompt: str, model: str = MODEL_NAME) -> str:\n",
    "    \"\"\"Call local LLM via Ollama.\"\"\"\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        # Fallback for demo purposes\n",
    "        return '{\"entities\": [], \"relationships\": []}'\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "def call_llm_json(prompt: str, model: str = MODEL_NAME) -> dict:\n",
    "    \"\"\"Call LLM and parse JSON response.\"\"\"\n",
    "    try:\n",
    "        if OLLAMA_AVAILABLE:\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                format=\"json\"\n",
    "            )\n",
    "            return json.loads(response['message']['content'])\n",
    "        else:\n",
    "            return {\"entities\": [], \"relationships\": []}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b261c",
   "metadata": {},
   "source": [
    "## 3. Data Models for Graph RAG\n",
    "\n",
    "We define structured classes to track:\n",
    "- **Entities**: Extracted from documents with source provenance\n",
    "- **Relationships**: Connections between entities\n",
    "- **Citations**: Verifiable links from claims to source sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"An entity extracted from a document with full provenance.\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    entity_type: str  # PERSON, ORGANIZATION, CONCEPT, TECHNOLOGY, etc.\n",
    "    description: str\n",
    "    source_doc: str\n",
    "    source_sentence: str  # Exact sentence where entity was found\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass \n",
    "class Relationship:\n",
    "    \"\"\"A relationship between two entities with provenance.\"\"\"\n",
    "    source_entity: str\n",
    "    target_entity: str\n",
    "    relation_type: str  # WORKS_FOR, DEVELOPED, LOCATED_IN, etc.\n",
    "    description: str\n",
    "    source_doc: str\n",
    "    source_sentence: str\n",
    "\n",
    "@dataclass\n",
    "class Citation:\n",
    "    \"\"\"A verifiable citation linking a claim to its source.\"\"\"\n",
    "    claim: str\n",
    "    source_document: str\n",
    "    source_sentence: str\n",
    "    confidence: float\n",
    "    graph_path: List[str]  # Path through the graph that led to this claim\n",
    "\n",
    "@dataclass\n",
    "class AttributedAnswer:\n",
    "    \"\"\"Final answer with full attribution.\"\"\"\n",
    "    answer: str\n",
    "    citations: List[Citation]\n",
    "    reasoning_trace: List[str]\n",
    "    entities_used: List[str]\n",
    "    relationships_traversed: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49746354",
   "metadata": {},
   "source": [
    "## 4. Knowledge Graph Manager\n",
    "\n",
    "The core class that:\n",
    "1. Extracts entities and relationships using LLM\n",
    "2. Builds a graph structure (NetworkX for demo, Neo4j for production)\n",
    "3. Maintains source provenance for every node and edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ecd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphRAG:\n",
    "    \"\"\"\n",
    "    Local Knowledge Graph RAG with Verifiable Attribution.\n",
    "    \n",
    "    This class demonstrates Graph RAG that:\n",
    "    1. Runs entirely locally (privacy-first)\n",
    "    2. Extracts entities and relationships using LLM\n",
    "    3. Enables multi-hop reasoning\n",
    "    4. Provides verifiable source attribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entities: Dict[str, Entity] = {}\n",
    "        self.relationships: List[Relationship] = []\n",
    "        self.documents: Dict[str, str] = {}\n",
    "        self.entity_embeddings: Dict[str, np.ndarray] = {}\n",
    "        \n",
    "    def _generate_id(self, text: str) -> str:\n",
    "        \"\"\"Generate a unique ID for an entity.\"\"\"\n",
    "        return hashlib.md5(text.lower().encode()).hexdigest()[:12]\n",
    "    \n",
    "    def extract_entities_and_relationships(\n",
    "        self, \n",
    "        text: str, \n",
    "        doc_name: str\n",
    "    ) -> Tuple[List[Entity], List[Relationship]]:\n",
    "        \"\"\"\n",
    "        Use LLM to extract entities and relationships from text.\n",
    "        Each extraction preserves the source sentence for attribution.\n",
    "        \"\"\"\n",
    "        \n",
    "        extraction_prompt = f\"\"\"Analyze this text and extract entities and relationships.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Return JSON with this exact structure:\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"name\": \"Entity Name\",\n",
    "      \"type\": \"PERSON|ORGANIZATION|CONCEPT|TECHNOLOGY|EVENT|LOCATION\",\n",
    "      \"description\": \"Brief description\",\n",
    "      \"source_sentence\": \"Exact sentence from the text where this entity appears\"\n",
    "    }}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {{\n",
    "      \"source\": \"Source Entity Name\",\n",
    "      \"target\": \"Target Entity Name\", \n",
    "      \"type\": \"WORKS_FOR|DEVELOPED|USES|LOCATED_IN|RELATED_TO|FOUNDED|MENTORED\",\n",
    "      \"description\": \"How they are related\",\n",
    "      \"source_sentence\": \"Exact sentence describing this relationship\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "IMPORTANT: \n",
    "- Include the EXACT source_sentence from the original text\n",
    "- This is critical for verifiable attribution\n",
    "\"\"\"\n",
    "        \n",
    "        result = call_llm_json(extraction_prompt)\n",
    "        \n",
    "        entities = []\n",
    "        for e in result.get('entities', []):\n",
    "            entity_id = self._generate_id(e['name'])\n",
    "            entity = Entity(\n",
    "                id=entity_id,\n",
    "                name=e['name'],\n",
    "                entity_type=e.get('type', 'UNKNOWN'),\n",
    "                description=e.get('description', ''),\n",
    "                source_doc=doc_name,\n",
    "                source_sentence=e.get('source_sentence', text[:100])\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        \n",
    "        relationships = []\n",
    "        for r in result.get('relationships', []):\n",
    "            rel = Relationship(\n",
    "                source_entity=r['source'],\n",
    "                target_entity=r['target'],\n",
    "                relation_type=r.get('type', 'RELATED_TO'),\n",
    "                description=r.get('description', ''),\n",
    "                source_doc=doc_name,\n",
    "                source_sentence=r.get('source_sentence', '')\n",
    "            )\n",
    "            relationships.append(rel)\n",
    "        \n",
    "        return entities, relationships\n",
    "    \n",
    "    def add_document(self, text: str, doc_name: str):\n",
    "        \"\"\"\n",
    "        Ingest a document into the knowledge graph.\n",
    "        Extracts entities and relationships, preserving source attribution.\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Processing document: {doc_name}\")\n",
    "        self.documents[doc_name] = text\n",
    "        \n",
    "        # Extract entities and relationships\n",
    "        entities, relationships = self.extract_entities_and_relationships(text, doc_name)\n",
    "        \n",
    "        # Add entities to graph\n",
    "        for entity in entities:\n",
    "            self.entities[entity.id] = entity\n",
    "            \n",
    "            # Generate embedding for vector search\n",
    "            embedding = embedder.encode(f\"{entity.name}: {entity.description}\")\n",
    "            self.entity_embeddings[entity.id] = embedding\n",
    "            entity.embedding = embedding\n",
    "            \n",
    "            # Add node to graph with all provenance metadata\n",
    "            self.graph.add_node(\n",
    "                entity.id,\n",
    "                name=entity.name,\n",
    "                type=entity.entity_type,\n",
    "                description=entity.description,\n",
    "                source_doc=entity.source_doc,\n",
    "                source_sentence=entity.source_sentence\n",
    "            )\n",
    "        \n",
    "        # Add relationships to graph\n",
    "        for rel in relationships:\n",
    "            source_id = self._generate_id(rel.source_entity)\n",
    "            target_id = self._generate_id(rel.target_entity)\n",
    "            \n",
    "            if source_id in self.entities and target_id in self.entities:\n",
    "                self.graph.add_edge(\n",
    "                    source_id,\n",
    "                    target_id,\n",
    "                    relation_type=rel.relation_type,\n",
    "                    description=rel.description,\n",
    "                    source_doc=rel.source_doc,\n",
    "                    source_sentence=rel.source_sentence\n",
    "                )\n",
    "                self.relationships.append(rel)\n",
    "        \n",
    "        print(f\"   ‚úÖ Extracted {len(entities)} entities and {len(relationships)} relationships\")\n",
    "        return entities, relationships\n",
    "    \n",
    "    def vector_search(self, query: str, top_k: int = 5) -> List[Entity]:\n",
    "        \"\"\"Find entities most similar to the query using vector search.\"\"\"\n",
    "        query_embedding = embedder.encode(query)\n",
    "        \n",
    "        similarities = []\n",
    "        for entity_id, embedding in self.entity_embeddings.items():\n",
    "            similarity = np.dot(query_embedding, embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)\n",
    "            )\n",
    "            similarities.append((entity_id, similarity))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top entities\n",
    "        return [self.entities[eid] for eid, _ in similarities[:top_k]]\n",
    "    \n",
    "    def multi_hop_traverse(\n",
    "        self, \n",
    "        start_entities: List[Entity], \n",
    "        max_hops: int = 2\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Perform multi-hop graph traversal from starting entities.\n",
    "        Returns all reachable nodes with their paths (for attribution).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        visited = set()\n",
    "        \n",
    "        for start_entity in start_entities:\n",
    "            # BFS traversal up to max_hops\n",
    "            queue = [(start_entity.id, [start_entity.name], 0)]\n",
    "            \n",
    "            while queue:\n",
    "                current_id, path, depth = queue.pop(0)\n",
    "                \n",
    "                if depth > max_hops or current_id in visited:\n",
    "                    continue\n",
    "                \n",
    "                visited.add(current_id)\n",
    "                \n",
    "                # Get node data\n",
    "                if current_id in self.graph:\n",
    "                    node_data = self.graph.nodes[current_id]\n",
    "                    results.append({\n",
    "                        \"entity_id\": current_id,\n",
    "                        \"entity_name\": node_data.get('name'),\n",
    "                        \"description\": node_data.get('description'),\n",
    "                        \"source_doc\": node_data.get('source_doc'),\n",
    "                        \"source_sentence\": node_data.get('source_sentence'),\n",
    "                        \"path\": path,\n",
    "                        \"depth\": depth\n",
    "                    })\n",
    "                    \n",
    "                    # Explore neighbors\n",
    "                    for neighbor in self.graph.neighbors(current_id):\n",
    "                        edge_data = self.graph.edges[current_id, neighbor]\n",
    "                        neighbor_name = self.graph.nodes[neighbor].get('name', neighbor)\n",
    "                        new_path = path + [f\"--[{edge_data.get('relation_type')}]-->\", neighbor_name]\n",
    "                        queue.append((neighbor, new_path, depth + 1))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(\n",
    "        self, \n",
    "        query: str, \n",
    "        vector_top_k: int = 3, \n",
    "        graph_hops: int = 2\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining vector similarity and graph traversal.\n",
    "        \n",
    "        1. Vector search finds entry points (most relevant entities)\n",
    "        2. Graph traversal expands to connected information\n",
    "        \"\"\"\n",
    "        print(f\"üîç Hybrid Search: '{query}'\")\n",
    "        \n",
    "        # Step 1: Vector search for entry points\n",
    "        entry_points = self.vector_search(query, top_k=vector_top_k)\n",
    "        print(f\"   üìå Found {len(entry_points)} entry points via vector search\")\n",
    "        \n",
    "        # Step 2: Multi-hop graph traversal\n",
    "        traversal_results = self.multi_hop_traverse(entry_points, max_hops=graph_hops)\n",
    "        print(f\"   üîó Traversed to {len(traversal_results)} related entities\")\n",
    "        \n",
    "        return traversal_results\n",
    "    \n",
    "    def generate_attributed_answer(\n",
    "        self, \n",
    "        query: str,\n",
    "        context: List[Dict[str, Any]]\n",
    "    ) -> AttributedAnswer:\n",
    "        \"\"\"\n",
    "        Generate an answer with verifiable source citations.\n",
    "        \n",
    "        Each claim in the answer is linked to:\n",
    "        - Source document\n",
    "        - Source sentence\n",
    "        - Graph path that led to this information\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build context string with source markers\n",
    "        context_parts = []\n",
    "        source_map = {}\n",
    "        \n",
    "        for i, item in enumerate(context):\n",
    "            source_key = f\"[{i+1}]\"\n",
    "            context_parts.append(\n",
    "                f\"{source_key} {item['entity_name']}: {item['description']}\"\n",
    "            )\n",
    "            source_map[source_key] = {\n",
    "                \"document\": item['source_doc'],\n",
    "                \"sentence\": item['source_sentence'],\n",
    "                \"path\": item['path']\n",
    "            }\n",
    "        \n",
    "        context_text = \"\\n\".join(context_parts)\n",
    "        \n",
    "        answer_prompt = f\"\"\"Based on this knowledge graph context, answer the question.\n",
    "CRITICAL: Cite your sources using [N] notation for each claim.\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Provide a comprehensive answer with inline citations like [1], [2] for each claim.\n",
    "\"\"\"\n",
    "        \n",
    "        answer_text = call_llm(answer_prompt)\n",
    "        \n",
    "        # Extract citations from the answer\n",
    "        import re\n",
    "        citation_refs = re.findall(r'\\[(\\d+)\\]', answer_text)\n",
    "        \n",
    "        citations = []\n",
    "        for ref in set(citation_refs):\n",
    "            key = f\"[{ref}]\"\n",
    "            if key in source_map:\n",
    "                src = source_map[key]\n",
    "                citations.append(Citation(\n",
    "                    claim=f\"Reference {key}\",\n",
    "                    source_document=src['document'],\n",
    "                    source_sentence=src['sentence'],\n",
    "                    confidence=0.9,\n",
    "                    graph_path=src['path']\n",
    "                ))\n",
    "        \n",
    "        # Build reasoning trace\n",
    "        reasoning_trace = [\n",
    "            f\"üîç Query: {query}\",\n",
    "            f\"üìå Found {len(context)} relevant entities\",\n",
    "            f\"üîó Traversed graph paths\",\n",
    "            f\"üìù Generated answer with {len(citations)} citations\"\n",
    "        ]\n",
    "        \n",
    "        return AttributedAnswer(\n",
    "            answer=answer_text,\n",
    "            citations=citations,\n",
    "            reasoning_trace=reasoning_trace,\n",
    "            entities_used=[c['entity_name'] for c in context],\n",
    "            relationships_traversed=[]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe269b",
   "metadata": {},
   "source": [
    "## 5. Ingest Sample Documents\n",
    "\n",
    "Let's demonstrate with sample documents that require **multi-hop reasoning** - something Vector RAG struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents designed to demonstrate multi-hop reasoning\n",
    "# Note: Vector RAG would need to find BOTH documents to answer questions\n",
    "# about how Dr. Smith's research relates to the quantum computing project\n",
    "\n",
    "SAMPLE_DOCUMENTS = {\n",
    "    \"research_team.txt\": \"\"\"\n",
    "Dr. Sarah Chen leads the Advanced AI Research Lab at TechCorp. \n",
    "She pioneered the development of the NeuraSparse algorithm in 2022.\n",
    "Dr. Chen previously studied under Professor James Miller at Stanford University.\n",
    "Professor Miller is known for his foundational work in neural network optimization.\n",
    "The Advanced AI Research Lab collaborates closely with the Quantum Computing Division.\n",
    "Dr. Chen's team includes 15 researchers working on efficient neural architectures.\n",
    "\"\"\",\n",
    "\n",
    "    \"quantum_project.txt\": \"\"\"\n",
    "The Quantum Computing Division at TechCorp is led by Dr. Marcus Williams.\n",
    "Dr. Williams implemented the NeuraSparse algorithm in their quantum error correction system.\n",
    "This implementation reduced error rates by 47% compared to traditional methods.\n",
    "The quantum team works on Project Aurora, funded by a $50 million grant.\n",
    "Project Aurora aims to build a fault-tolerant quantum computer by 2027.\n",
    "The project uses insights from Professor Miller's optimization research.\n",
    "\"\"\",\n",
    "\n",
    "    \"company_overview.txt\": \"\"\"\n",
    "TechCorp was founded in 2015 by Elena Rodriguez in San Francisco.\n",
    "The company has grown to over 500 employees across three divisions.\n",
    "TechCorp's three main divisions are: AI Research, Quantum Computing, and Cloud Services.\n",
    "Elena Rodriguez previously worked at Google as a principal engineer.\n",
    "The company's headquarters are located in the South Bay area.\n",
    "TechCorp has partnerships with MIT and Stanford University for research collaboration.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Initialize the Knowledge Graph RAG\n",
    "kg_rag = KnowledgeGraphRAG()\n",
    "\n",
    "# Ingest all documents\n",
    "for doc_name, content in SAMPLE_DOCUMENTS.items():\n",
    "    kg_rag.add_document(content, doc_name)\n",
    "\n",
    "print(f\"\\nüìä Graph Statistics:\")\n",
    "print(f\"   Total Entities: {len(kg_rag.entities)}\")\n",
    "print(f\"   Total Relationships: {len(kg_rag.relationships)}\")\n",
    "print(f\"   Graph Nodes: {kg_rag.graph.number_of_nodes()}\")\n",
    "print(f\"   Graph Edges: {kg_rag.graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2b567",
   "metadata": {},
   "source": [
    "## 6. Visualize the Knowledge Graph\n",
    "\n",
    "Let's see the structure of our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display entities\n",
    "print(\"üîµ ENTITIES IN KNOWLEDGE GRAPH:\\n\")\n",
    "for entity_id, entity in kg_rag.entities.items():\n",
    "    print(f\"  [{entity.entity_type}] {entity.name}\")\n",
    "    print(f\"       üìÑ Source: {entity.source_doc}\")\n",
    "    print(f\"       üìù \\\"{entity.source_sentence[:80]}...\\\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó RELATIONSHIPS IN KNOWLEDGE GRAPH:\\n\")\n",
    "for rel in kg_rag.relationships:\n",
    "    print(f\"  {rel.source_entity} --[{rel.relation_type}]--> {rel.target_entity}\")\n",
    "    print(f\"       üìÑ Source: {rel.source_doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ed424",
   "metadata": {},
   "source": [
    "## 7. Hybrid Search: Vector + Graph Traversal\n",
    "\n",
    "This is where Graph RAG shines. We combine:\n",
    "1. **Vector Search**: Find relevant entry points\n",
    "2. **Graph Traversal**: Expand to connected information (multi-hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hop question that requires connecting information from multiple documents\n",
    "MULTI_HOP_QUERY = \"How does Professor Miller's research influence the quantum computing project?\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ MULTI-HOP REASONING DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì Query: {MULTI_HOP_QUERY}\")\n",
    "print(\"\\n‚ö†Ô∏è  Why Vector RAG would struggle:\")\n",
    "print(\"   - 'Professor Miller' appears in research_team.txt\")\n",
    "print(\"   - 'quantum computing project' details are in quantum_project.txt\")\n",
    "print(\"   - Vector RAG finds similar chunks, but may miss the CONNECTION\")\n",
    "print()\n",
    "\n",
    "# Perform hybrid search\n",
    "context = kg_rag.hybrid_search(\n",
    "    MULTI_HOP_QUERY, \n",
    "    vector_top_k=3, \n",
    "    graph_hops=2\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Retrieved {len(context)} relevant pieces of information:\")\n",
    "for i, item in enumerate(context):\n",
    "    print(f\"\\n  [{i+1}] {item['entity_name']}\")\n",
    "    print(f\"      Path: {' '.join(item['path'])}\")\n",
    "    print(f\"      Source: {item['source_doc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215a28c",
   "metadata": {},
   "source": [
    "## 8. Generate Answer with Verifiable Citations\n",
    "\n",
    "The key differentiator: every claim is traced back to its **exact source sentence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b282d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attributed answer\n",
    "result = kg_rag.generate_attributed_answer(MULTI_HOP_QUERY, context)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìù GENERATED ANSWER WITH CITATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{result.answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç REASONING TRACE\")\n",
    "print(\"=\"*70)\n",
    "for step in result.reasoning_trace:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö VERIFIABLE CITATIONS\")\n",
    "print(\"=\"*70)\n",
    "for i, citation in enumerate(result.citations):\n",
    "    print(f\"\\n  Citation {i+1}:\")\n",
    "    print(f\"    üìÑ Document: {citation.source_document}\")\n",
    "    print(f\"    üìù Source Sentence: \\\"{citation.source_sentence}\\\"\")\n",
    "    print(f\"    üîó Graph Path: {' ‚Üí '.join(citation.graph_path)}\")\n",
    "    print(f\"    ‚úì Confidence: {citation.confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78100b7",
   "metadata": {},
   "source": [
    "## 9. Additional Multi-Hop Query Examples\n",
    "\n",
    "Let's test more queries that demonstrate the power of graph-based reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40684619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More multi-hop queries\n",
    "ADDITIONAL_QUERIES = [\n",
    "    \"What is the relationship between Dr. Chen and Project Aurora?\",\n",
    "    \"How did the NeuraSparse algorithm impact quantum error correction?\",\n",
    "    \"Who founded the company where Dr. Chen works?\"\n",
    "]\n",
    "\n",
    "for query in ADDITIONAL_QUERIES:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚ùì {query}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    context = kg_rag.hybrid_search(query, vector_top_k=3, graph_hops=2)\n",
    "    result = kg_rag.generate_attributed_answer(query, context)\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer:\\n{result.answer[:500]}...\")\n",
    "    print(f\"\\nüìö Citations: {len(result.citations)} sources verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d1e2f",
   "metadata": {},
   "source": [
    "## 10. Comparison: Graph RAG vs Vector RAG\n",
    "\n",
    "| Aspect | Vector RAG | Graph RAG |\n",
    "|--------|------------|-----------|\n",
    "| **Multi-hop Questions** | ‚ùå Struggles to connect disjoint facts | ‚úÖ Follows relationship edges |\n",
    "| **Attribution Granularity** | üìÑ Chunk-level | üìù Sentence-level |\n",
    "| **Relationship Discovery** | ‚ùå Implicit in embeddings | ‚úÖ Explicit, traversable |\n",
    "| **Privacy** | ‚ö†Ô∏è Often requires cloud APIs | ‚úÖ Fully local with Ollama |\n",
    "| **Reasoning Transparency** | ‚ùå Black box | ‚úÖ Visible graph paths |\n",
    "\n",
    "### When to Use Graph RAG\n",
    "\n",
    "‚úÖ **Use Graph RAG when:**\n",
    "- Questions require connecting facts from multiple documents\n",
    "- You need verifiable, sentence-level attribution  \n",
    "- Relationship discovery is important\n",
    "- Privacy is critical (on-premise deployment)\n",
    "\n",
    "‚ö†Ô∏è **Stick with Vector RAG when:**\n",
    "- Simple factoid retrieval suffices\n",
    "- Documents have minimal interconnections\n",
    "- Speed is more important than attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d11ef",
   "metadata": {},
   "source": [
    "## 11. Production Deployment with Neo4j\n",
    "\n",
    "For production use, replace NetworkX with Neo4j for:\n",
    "- Scalability to millions of entities\n",
    "- ACID transactions\n",
    "- Native graph query language (Cypher)\n",
    "- Visualization tools\n",
    "\n",
    "```python\n",
    "# Production example with Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Add entity with provenance\n",
    "with driver.session() as session:\n",
    "    session.run(\"\"\"\n",
    "        MERGE (e:Entity {id: $id})\n",
    "        SET e.name = $name,\n",
    "            e.source_doc = $source_doc,\n",
    "            e.source_sentence = $source_sentence\n",
    "    \"\"\", id=entity_id, name=name, source_doc=doc, source_sentence=sentence)\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "- **VeritasGraph**: [GitHub Repository](https://github.com/bibinprathap/VeritasGraph)\n",
    "- **Microsoft GraphRAG**: Graph-based RAG research\n",
    "- **Ollama**: Local LLM inference\n",
    "- **Neo4j**: Graph database for production deployments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
