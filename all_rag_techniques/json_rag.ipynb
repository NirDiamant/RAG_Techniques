{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b48b7a0-38a7-492d-ab35-51f7a8f29d32",
   "metadata": {},
   "source": [
    "# Tutorial Overview: Building a Semantic Search System from JSON Data with jRAG\n",
    "\n",
    "This tutorial walks through the process of creating a semantic search system that can find relevant JSON documents based on a natural language query using [jRAG](https://pypi.org/project/jrag/) - A tool for generating embedding strings from JSON/Dictionary fields.\n",
    "\n",
    "This is necessary because we can't embed JSONs as they are, we need to create a string representation to embed. It is possible to use `json.dumps()` to do this, however we might not always want to embed the entire JSON (only relevant fields) but we do still want to retrieve the entire JSON. jRAG helps simplify this.\n",
    "\n",
    "jRAG is built on [jsonpath-ng](https://pypi.org/project/jsonpath-ng/) so refer to this documentation for the expressions used as part of the configs (look for cell \"Combine fields from JSON data\" for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c064cb-006a-401c-97b0-b030fde571f8",
   "metadata": {},
   "source": [
    "# Step by Step\n",
    "\n",
    "Here's a breakdown of the steps demonstrated:\n",
    "\n",
    "1. Data Preparation (JSON): The tutorial starts with a list of Python dictionaries (json_lst), simulating a collection of JSON documents. Each document contains structured information like titles, authors, abstracts, tags, etc.\n",
    "\n",
    "2. Text Conversion with jrag:\n",
    "    * Challenge: Embedding models (like Sentence Transformers) work best with text, not raw JSON structures.\n",
    "    * Solution: The jrag library is used to convert the structured JSON data into meaningful text strings suitable for embedding.\n",
    "    * Configuration: A jrag_config dictionary is defined, mapping descriptive labels (like 'Title', 'Abstract') to jsonpath-ng expressions. This tells jrag exactly which fields from the JSON to extract and combine.\n",
    "    * Execution: jrag.tag_list(json_lst, jrag_config) is called. This iterates through the list of dictionaries, applies the configured extraction rules to each, and adds a new key (defaulting to 'jrag_text') containing the resulting flattened text string to each dictionary.\n",
    "\n",
    "\n",
    "3. Corpus Creation: The generated text strings (jrag_text) are extracted from the modified dictionaries to form the corpus_texts. Crucially, a parallel list (corpus_metadata) is kept to store references (or the full original items) back to the original JSON data, allowing retrieval of the full document later.\n",
    "\n",
    "4. Embedding Generation:\n",
    "    * A pre-trained SentenceTransformer model (all-MiniLM-L6-v2) is loaded.\n",
    "    * This model is used to convert each text string in corpus_texts into a high-dimensional numerical vector (embedding). These embeddings capture the semantic meaning of the text.\n",
    "\n",
    "5. Vector Indexing with FAISS:\n",
    "    * Facebook AI Similarity Search (faiss) is used to create an efficient index (IndexFlatL2) for the generated embeddings.\n",
    "    * The corpus embeddings are added to this index. FAISS allows for very fast searching over large numbers of vectors.\n",
    "\n",
    "6. Querying:\n",
    "    * A sample text query (\"Tell me about semantic search technologies.\") is defined.\n",
    "    * The same Sentence Transformer model is used to convert the query text into its own embedding vector.\n",
    "    * The FAISS index's search method is used to find the embeddings in the index that are most similar (closest in vector space, using L2 distance here) to the query embedding.\n",
    "\n",
    "7. Retrieval and Display:\n",
    "    * The search returns the indices of the top N most similar items in the original corpus.\n",
    "    * Using the corpus_metadata list created earlier, the indices are mapped back to the original JSON documents.\n",
    "    * The tutorial then prints the details of these retrieved JSON documents, demonstrating that the system successfully found the entries semantically related to the query.\n",
    "\n",
    "In essence, the tutorial showcases how jrag acts as a bridge transforming structured JSON data into a text format that can be understood and processed by modern NLP tools for tasks like semantic search and retrieval, forming the core \"retrieval\" part of a RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c818657-d530-4af7-a97a-926bb344c952",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f66904-1c0c-46d9-8b6c-940d3cd86656",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu numpy jrag --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dab18-5b53-4396-ab78-ce2caac55f66",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5aeca-c1f9-4868-9cbe-6a836387076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import jrag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1a19d-cf0b-4755-8d8e-24b29d578f2b",
   "metadata": {},
   "source": [
    "# JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f4195-d948-4a15-9cd9-15c91dd39d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lst = [\n",
    "  {\n",
    "    \"id\": \"report_tech_001\",\n",
    "    \"title\": \"FAISS Library Analysis\",\n",
    "    \"author\": \"Alice Smith\",\n",
    "    \"timestamp\": \"2025-04-10T10:00:00Z\",\n",
    "    \"category\": \"tech\",\n",
    "    \"status\": \"published\",\n",
    "    \"abstract\": \"An in-depth look at FAISS, a library developed by Facebook AI for efficient similarity search and clustering of dense vectors. We explore its various index types and performance characteristics.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"Internal Research\",\n",
    "      \"confidence\": 0.95,\n",
    "      \"word_count\": 450,\n",
    "      \"tags\": [\"ai\", \"similarity search\", \"vector database\", \"facebook ai\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"news_astro_002\",\n",
    "    \"title\": \"JWST Captures New Nebula\",\n",
    "    \"author\": \"Bob Johnson\",\n",
    "    \"timestamp\": \"2025-04-11T14:30:00Z\",\n",
    "    \"category\": \"science\",\n",
    "    \"status\": \"published\",\n",
    "    \"abstract\": \"The James Webb Space Telescope has delivered breathtaking new imagery of the 'Cosmic Cliffs' region within the Carina Nebula, revealing previously hidden star formation.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"NASA Press Release\",\n",
    "      \"confidence\": 0.99,\n",
    "      \"image_ref\": \"jwst_carina_01.jpg\",\n",
    "      \"tags\": [\"astronomy\", \"jwst\", \"telescope\", \"space\", \"nebula\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"paper_nlp_003\",\n",
    "    \"title\": \"Sentence Embedding Techniques\",\n",
    "    \"author\": \"Carol Williams\",\n",
    "    \"timestamp\": \"2025-04-12T09:15:00Z\",\n",
    "    \"category\": \"tech\",\n",
    "    \"status\": \"published\",\n",
    "    \"abstract\": \"This paper reviews modern techniques for computing meaningful sentence and text embeddings, focusing on transformer-based models like those provided by the Sentence Transformers library.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"AI Conference Proc.\",\n",
    "      \"doi\": \"10.1234/aiconf.2025.5678\",\n",
    "      \"word_count\": 8500,\n",
    "      \"tags\": [\"nlp\", \"ai\", \"embeddings\", \"sentence transformers\", \"deep learning\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"brief_quantum_004\",\n",
    "    \"title\": \"Quantum Computing Update\",\n",
    "    \"author\": \"David Brown\",\n",
    "    \"timestamp\": \"2025-04-12T16:00:00Z\",\n",
    "    \"category\": \"news\",\n",
    "    \"status\": \"draft\",\n",
    "    \"abstract\": \"Recent advancements in qubit stability mark a significant step forward for practical quantum computing applications. Researchers highlight potential impacts on cryptography and materials science.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"Tech Journal X\",\n",
    "      \"confidence\": 0.88,\n",
    "      \"tags\": [\"quantum computing\", \"technology\", \"research\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"howto_ai_005\",\n",
    "    \"title\": \"Scalable Semantic Search Guide\",\n",
    "    \"author\": \"Alice Smith\",\n",
    "    \"timestamp\": \"2025-04-13T11:00:00Z\",\n",
    "    \"category\": \"tech\",\n",
    "    \"status\": \"published\",\n",
    "    \"abstract\": \"A practical guide demonstrating how to combine Sentence Transformers for embedding generation and FAISS for indexing to build scalable semantic search systems capable of handling large datasets.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"Tech Blog\",\n",
    "      \"difficulty\": \"intermediate\",\n",
    "      \"word_count\": 1200,\n",
    "      \"tags\": [\"semantic search\", \"ai\", \"faiss\", \"embeddings\", \"tutorial\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"discovery_bio_006\",\n",
    "    \"title\": \"New Deep-Sea Species\",\n",
    "    \"author\": \"Eva Green\",\n",
    "    \"timestamp\": \"2025-04-09T08:45:00Z\",\n",
    "    \"category\": \"science\",\n",
    "    \"status\": \"published\",\n",
    "    \"abstract\": \"Marine biologists participating in the 'Ocean Depths' expedition have officially classified a new species of bioluminescent fish found near hydrothermal vents in the Pacific Ocean.\",\n",
    "    \"metadata\": {\n",
    "      \"source\": \"Journal of Marine Biology\",\n",
    "      \"confidence\": 0.97,\n",
    "      \"location\": \"Mariana Trench Region\",\n",
    "      \"tags\": [\"biology\", \"marine life\", \"discovery\", \"oceanography\"]\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290ed2c-1ae5-4644-9a20-eb01c34ef13e",
   "metadata": {},
   "source": [
    "# Combine fields from JSON data\n",
    "\n",
    "Using the most contextually relevant data to the usecase, we merge these fields into a single string that we use to create the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22af81b-d6c7-451a-acf4-c54600d8a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick which columns to combine using jsonpath-ng expressions\n",
    "jrag_config = {\n",
    "    'Title': '$.title',\n",
    "    'Author': '$.author',\n",
    "    'Category': '$.category',\n",
    "    'Tags': '$.metadata.tags[*]',  # Select all inside list\n",
    "    'Abstract': '$.abstract'\n",
    "}\n",
    "\n",
    "# tag_list adds the new combined field to the json\n",
    "json_lst = jrag.tag_list(json_lst, jrag_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fbd7e-aac5-4267-9f8b-75b89733a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first example\n",
    "first_json = json_lst[0]\n",
    "first_json['jrag_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b0d9f-d7cb-4680-adb5-3d6629b4a1e7",
   "metadata": {},
   "source": [
    "# Build corpus\n",
    "\n",
    "Here we create the embedding and vector store using SentenceTransformer and FAISS (all built locally, no API key needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c726c1d-f97d-4b14-b63b-c5b91a7e6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text content and keep track of original data reference\n",
    "# We store the original index to map FAISS results back to our JSON objects\n",
    "corpus_texts = []\n",
    "corpus_metadata = [] # To store original dicts or just IDs\n",
    "\n",
    "for i, item in enumerate(json_lst):\n",
    "    jrag_text = item.get('jrag_text')\n",
    "    if jrag_text and isinstance(jrag_text, str):\n",
    "        corpus_texts.append(jrag_text)\n",
    "        # Store the original item or just its ID for later retrieval\n",
    "        # Storing the whole item is easier for this example\n",
    "        corpus_metadata.append({\"original_index\": i, \"data\": item})\n",
    "    else:\n",
    "        print(f\"Warning: Item at index {i} is missing 'content' key or it's not a string. Skipping.\")\n",
    "\n",
    "if not corpus_texts:\n",
    "    print(\"Error: No valid text content found in the JSON data.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loaded {len(corpus_texts)} items with text content.\")# --- Configuration ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2' # A good & fast general-purpose model\n",
    "NUM_NEIGHBORS = 3 # How many similar items to retrieve\n",
    "\n",
    "# --- 2. Load Sentence Transformer Model ---\n",
    "print(f\"Loading Sentence Transformer model '{MODEL_NAME}'...\")\n",
    "start_time = time.time()\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "end_time = time.time()\n",
    "print(f\"Model loaded in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3. Generate Embeddings ---\n",
    "print(\"Generating embeddings for the corpus...\")\n",
    "start_time = time.time()\n",
    "# Ensure convert_to_numpy=True for FAISS compatibility\n",
    "corpus_embeddings = model.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "end_time = time.time()\n",
    "print(f\"Embeddings generated in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# FAISS requires float32 type\n",
    "corpus_embeddings = corpus_embeddings.astype('float32')\n",
    "\n",
    "# Get the dimensionality of embeddings (required by FAISS)\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# --- 4. Build FAISS Index ---\n",
    "# Using IndexFlatL2 - simple baseline, performs exhaustive search\n",
    "# L2 distance = Euclidean distance\n",
    "print(\"Building FAISS index (IndexFlatL2)...\")\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# --- 5. Add Embeddings to Index ---\n",
    "print(f\"Adding {len(corpus_embeddings)} embeddings to the index...\")\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"Index contains {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c98df-93b1-4c1e-8dce-44afcbaf6f3d",
   "metadata": {},
   "source": [
    "# Query\n",
    "\n",
    "We can now query `\"Tell me about semantic search technologies.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e9352-23a4-4187-bc6e-78e8d0368a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Prepare and Perform Search ---\n",
    "query_text = \"Tell me about semantic search technologies.\"\n",
    "print(f\"\\nPerforming search for query: '{query_text}'\")\n",
    "print(f\"Finding top {NUM_NEIGHBORS} similar items...\")\n",
    "\n",
    "# Generate embedding for the query\n",
    "start_time = time.time()\n",
    "query_embedding = model.encode([query_text], convert_to_numpy=True)\n",
    "query_embedding = query_embedding.astype('float32')\n",
    "end_time = time.time()\n",
    "print(f\"Query embedding generated in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Perform the search\n",
    "start_time = time.time()\n",
    "# The search function returns distances and indices (IDs) of neighbors\n",
    "# query_embedding needs to be 2D array (even if it's just one query)\n",
    "distances, indices = index.search(query_embedding, NUM_NEIGHBORS)\n",
    "end_time = time.time()\n",
    "print(f\"Search completed in {end_time - start_time:.4f} seconds.\")\n",
    "\n",
    "# --- 7. Display Results ---\n",
    "print(\"\\nSearch Results:\")\n",
    "print(\"--------------\")\n",
    "\n",
    "# indices[0] contains the results for the first (and only) query\n",
    "# distances[0] contains the corresponding distances\n",
    "if not indices[0].size:\n",
    "    print(\"No results found.\")\n",
    "else:\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        \n",
    "        # Map the index `idx` from FAISS back to our original data\n",
    "        # This works because we added embeddings in the same order as corpus_metadata\n",
    "        original_item_info = corpus_metadata[idx]\n",
    "        original_item = original_item_info['data']\n",
    "        distance = distances[0][i]\n",
    "\n",
    "        print(f\"Rank {i+1}:\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  ID: {original_item.get('id', 'N/A')}\")\n",
    "        print(f\"  Category: {original_item.get('category', 'N/A')}\")\n",
    "        print(f\"  Content: {original_item}\")\n",
    "        print(\"-\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
