{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b48b7a0-38a7-492d-ab35-51f7a8f29d32",
   "metadata": {},
   "source": [
    "# Building a Smart Search for JSON Data: A Beginner's Guide\n",
    "Imagine you have a lot of information stored in JSON files (a common format for structured data). Standard searching might only find exact word matches. What if you want to search based on the meaning or concept behind your query, not just keywords? That's where semantic search comes in!\n",
    "\n",
    "This tutorial will show you how to build a simple semantic search system using your JSON data. We'll use a helpful tool called jrag to prepare the data, turn text into special numerical representations called \"embeddings,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c064cb-006a-401c-97b0-b030fde571f8",
   "metadata": {},
   "source": [
    "## Why Do We Need This?\n",
    "AI models we use for semantic search understand numbers much better than raw text or complex JSON structures. So, our main challenge is to convert our JSON data into meaningful text and then into these numerical representations (embeddings) that capture the text's meaning.\n",
    "\n",
    "While you could just flatten a whole JSON into one big string, that often includes irrelevant information. The [jRAG](https://pypi.org/project/jrag/) library helps us carefully select and combine only the important fields from our JSON into a clean text string, perfect for creating high-quality embeddings. jRAG uses [jsonpath-ng](https://pypi.org/project/jsonpath-ng/) expressions to precisely target the data you want to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c818657-d530-4af7-a97a-926bb344c952",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f66904-1c0c-46d9-8b6c-940d3cd86656",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu numpy jrag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dab18-5b53-4396-ab78-ce2caac55f66",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e5aeca-c1f9-4868-9cbe-6a836387076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corma\\venv\\jrag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import jrag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fac66-6fc8-44b7-a2b8-12f4bf3b6681",
   "metadata": {},
   "source": [
    "# JSON Data - Nobel Prizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1a19d-cf0b-4755-8d8e-24b29d578f2b",
   "metadata": {},
   "source": [
    "## Download \n",
    "\n",
    "Before we can process data, we need some data. This cell uses the requests library to download a sample dataset - information about Nobel Prizes - from a public web address. It then parses the downloaded JSON data into a Python variable (a dictionary containing a list called 'prizes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9f4195-d948-4a15-9cd9-15c91dd39d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched and parsed JSON data.\n",
      "Number of events found: 676\n",
      "\n",
      "First 3 records:\n",
      " [\n",
      "  {\n",
      "    \"year\": \"2024\",\n",
      "    \"category\": \"chemistry\",\n",
      "    \"laureates\": [\n",
      "      {\n",
      "        \"id\": \"1039\",\n",
      "        \"firstname\": \"David\",\n",
      "        \"surname\": \"Baker\",\n",
      "        \"motivation\": \"\\\"for computational protein design\\\"\",\n",
      "        \"share\": \"2\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"1040\",\n",
      "        \"firstname\": \"Demis\",\n",
      "        \"surname\": \"Hassabis\",\n",
      "        \"motivation\": \"\\\"for protein structure prediction\\\"\",\n",
      "        \"share\": \"4\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"1041\",\n",
      "        \"firstname\": \"John\",\n",
      "        \"surname\": \"Jumper\",\n",
      "        \"motivation\": \"\\\"for protein structure prediction\\\"\",\n",
      "        \"share\": \"4\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"year\": \"2024\",\n",
      "    \"category\": \"economics\",\n",
      "    \"laureates\": [\n",
      "      {\n",
      "        \"id\": \"1044\",\n",
      "        \"firstname\": \"Daron\",\n",
      "        \"surname\": \"Acemoglu\",\n",
      "        \"motivation\": \"\\\"for studies of how institutions are formed and affect prosperity\\\"\",\n",
      "        \"share\": \"3\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"1045\",\n",
      "        \"firstname\": \"Simon\",\n",
      "        \"surname\": \"Johnson\",\n",
      "        \"motivation\": \"\\\"for studies of how institutions are formed and affect prosperity\\\"\",\n",
      "        \"share\": \"3\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"1046\",\n",
      "        \"firstname\": \"James\",\n",
      "        \"surname\": \"Robinson\",\n",
      "        \"motivation\": \"\\\"for studies of how institutions are formed and affect prosperity\\\"\",\n",
      "        \"share\": \"3\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"year\": \"2024\",\n",
      "    \"category\": \"literature\",\n",
      "    \"laureates\": [\n",
      "      {\n",
      "        \"id\": \"1042\",\n",
      "        \"firstname\": \"Kang\",\n",
      "        \"surname\": \"Han\",\n",
      "        \"motivation\": \"\\\"for her intense poetic prose that confronts historical traumas and exposes the fragility of human life\\\"\",\n",
      "        \"share\": \"1\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.nobelprize.org/v1/prize.json\"\n",
    "\n",
    "try:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url, timeout=15)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    response.raise_for_status() \n",
    "    # Parse the JSON data from the response\n",
    "    data = response.json()\n",
    "\n",
    "    print(\"Successfully fetched and parsed JSON data.\")\n",
    "\n",
    "    if isinstance(data, dict) and 'prizes' in data and isinstance(data['prizes'], list):\n",
    "         print(f\"Number of events found: {len(data['prizes'])}\")\n",
    "         if len(data['prizes']) > 0:\n",
    "              print(\"\\nFirst 3 records:\\n\", json.dumps(data['prizes'][:3], indent=2))\n",
    "    else:\n",
    "         print(\"The data structure might be different than expected.\")\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"The request to {url} timed out.\")\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"Could not connect to {url}. Please check the URL and your connection.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTP Error occurred: {e.response.status_code} {e.response.reason}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error during requests to {url}: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON. The response might not be valid JSON.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bfa84-133d-4392-8093-3f736dcca522",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "In this specific dataset, Nobel laureates (winners) might have their first and last names in separate fields (`firstname`, `surname`). For better searching, this cell loops through the prize data. For each laureate, it checks if both name fields exist and combines them into a single `full_name` field. It also handles cases where the full name might already be in the firstname field or if names are missing. This makes the name information more consistent for `jrag` to process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d52978d-1aea-4edb-9f0f-8df8ec8b59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prizes_lst = data['prizes']\n",
    "\n",
    "for prize_info in prizes_lst:\n",
    "    laureates_list = prize_info.get('laureates', [])\n",
    "\n",
    "    # Iterate through each laureate dictionary in the laureates list\n",
    "    for laureate in laureates_list:\n",
    "        # Check if both firstname and surname exist before combining\n",
    "        if 'firstname' in laureate and 'surname' in laureate:\n",
    "            # Create the full name string using an f-string\n",
    "            first_name = laureate['firstname']\n",
    "            last_name = laureate['surname']\n",
    "            full_name = f\"{first_name} {last_name}\"\n",
    "\n",
    "            # Add the new 'full_name' key-value pair to the laureate dictionary\n",
    "            laureate['full_name'] = full_name\n",
    "        # Some instances have the full name in the firstname field\n",
    "        elif 'surname' not in laureate and len(laureate['firstname'].split(' ')) > 1:\n",
    "            laureate['full_name'] = laureate['firstname']\n",
    "        else:\n",
    "            # Optional: Handle cases where names might be missing\n",
    "            print(f\"Warning: Missing 'firstname' or 'surname' for laureate ID {laureate.get('id', 'N/A')}:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb607fec-e6ae-4b38-af11-d921492d87ee",
   "metadata": {},
   "source": [
    "# Text to be embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290ed2c-1ae5-4644-9a20-eb01c34ef13e",
   "metadata": {},
   "source": [
    "## Single Example\n",
    "\n",
    "This cell demonstrates how `jrag` works on a single JSON record before we apply it to all of them.\n",
    "\n",
    "1. `jrag_config` Dictionary: We define a Python dictionary called `jrag_config`. The keys of this dictionary are descriptive labels (like 'Year', 'Category', 'Laureats', 'Motivation'). The values are special strings called `jsonpath-ng` expressions (e.g., `$.year`, `$.laureates[*].full_name`). These expressions tell jrag exactly where to find the corresponding data within the JSON structure. For instance, `$.laureates[*].full_name` means \"go into the 'laureates' list, look at every item (*), and get the 'full_name' field from each.\"\n",
    "2. `jrag.to_text()`: We take the first Nobel prize record (first_record) and pass it, along with our jrag_config, to the `jrag.to_text()` function.\n",
    "3. Output: This function extracts the data specified by the config, formats it nicely using the labels we provided (e.g., \"Year: 2023, Category: Physics, ...\"), and combines it into a single text string (example_text). This string is what we would potentially turn into an embedding. The cell prints this example text and the original JSON it came from, so you can see the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22af81b-d6c7-451a-acf4-c54600d8a920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to embed:\n",
      "\"Year: 2024 | Category: chemistry | Laureats: [David Baker, Demis Hassabis, John Jumper] | Motivation: \"for computational protein design\"\"\n",
      "\n",
      "Built from:\n",
      "{\n",
      "  \"year\": \"2024\",\n",
      "  \"category\": \"chemistry\",\n",
      "  \"laureates\": [\n",
      "    {\n",
      "      \"id\": \"1039\",\n",
      "      \"firstname\": \"David\",\n",
      "      \"surname\": \"Baker\",\n",
      "      \"motivation\": \"\\\"for computational protein design\\\"\",\n",
      "      \"share\": \"2\",\n",
      "      \"full_name\": \"David Baker\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"1040\",\n",
      "      \"firstname\": \"Demis\",\n",
      "      \"surname\": \"Hassabis\",\n",
      "      \"motivation\": \"\\\"for protein structure prediction\\\"\",\n",
      "      \"share\": \"4\",\n",
      "      \"full_name\": \"Demis Hassabis\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"1041\",\n",
      "      \"firstname\": \"John\",\n",
      "      \"surname\": \"Jumper\",\n",
      "      \"motivation\": \"\\\"for protein structure prediction\\\"\",\n",
      "      \"share\": \"4\",\n",
      "      \"full_name\": \"John Jumper\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "first_record = prizes_lst[0]\n",
    "\n",
    "jrag_config = {\n",
    "    'Year': '$.year',\n",
    "    'Category': '$.category',\n",
    "    'Laureats': '$.laureates[*].full_name',\n",
    "    'Motivation': '$.laureates[0].motivation'  # Here we only return the motivation field of the first instance ($.laureates[0]) as they're all the same\n",
    "}\n",
    "\n",
    "example_text = jrag.to_text(first_record, jrag_config)\n",
    "print(f'Text to embed:\\n\"{example_text}\"')\n",
    "print('\\nBuilt from:')\n",
    "print(json.dumps(first_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371778f-c115-4041-95df-3e0de71bb2f2",
   "metadata": {},
   "source": [
    "## Add to all records\n",
    "\n",
    "We now run the same process over all records in the JSON using `jrag.tag_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9def463-da7e-4fba-92d2-d95138c49c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2024 | Category: chemistry | Laureats: [David Baker, Demis Hassabis, John Jumper] | Motivation: \"for computational protein design\"\n"
     ]
    }
   ],
   "source": [
    "jrag_config = {\n",
    "    'Year': '$.year',\n",
    "    'Category': '$.category',\n",
    "    'Laureats': '$.laureates[*].full_name',\n",
    "    'Motivation': '$.laureates[0].motivation'\n",
    "}\n",
    "\n",
    "prizes_tagged_lst = jrag.tag_list(prizes_lst, jrag_config)\n",
    "\n",
    "# Inspect first example\n",
    "print(prizes_tagged_lst[0]['jrag_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b0d9f-d7cb-4680-adb5-3d6629b4a1e7",
   "metadata": {},
   "source": [
    "# Build corpus\n",
    "\n",
    "We need two things to proceed: the text we want to embed and a way to link back to the original data. This cell creates two lists:\n",
    "\n",
    "1. `corpus_texts`: It loops through our `prizes_tagged_lst`. For each item, it takes the text string stored in the 'jrag_text' key and adds it to the `corpus_texts` list. This list will be fed into the embedding model.\n",
    "2. `corpus_metadat`a: At the same time, it adds the entire original JSON object (which now includes the 'jrag_text') to the corpus_metadata list. This list runs parallel to `corpus_texts`. So, the first text in corpus_texts corresponds to the first JSON object in corpus_metadata, the second text to the second JSON, and so on. This link is crucial for retrieving the full JSON details after finding a match using the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c726c1d-f97d-4b14-b63b-c5b91a7e6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 676 items with text content.\n"
     ]
    }
   ],
   "source": [
    "# Extract the text content and keep track of original data reference\n",
    "# We store the original index to map FAISS results back to our JSON objects\n",
    "corpus_texts = []\n",
    "corpus_metadata = [] # To store original dicts or just IDs\n",
    "for i, item in enumerate(prizes_tagged_lst):\n",
    "    jrag_text = item.get('jrag_text')\n",
    "    if jrag_text and isinstance(jrag_text, str):\n",
    "        corpus_texts.append(jrag_text)\n",
    "        # Store the original item or just its ID for later retrieval\n",
    "        # Storing the whole item is easier for this example\n",
    "        corpus_metadata.append({\"original_index\": i, \"data\": item})\n",
    "    else:\n",
    "        print(f\"Warning: Item at index {i} is missing 'content' key or it's not a string. Skipping.\")\n",
    "\n",
    "if not corpus_texts:\n",
    "    print(\"Error: No valid text content found in the JSON data.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loaded {len(corpus_texts)} items with text content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4367041-8010-4630-9be2-5cc8a633b521",
   "metadata": {},
   "source": [
    "# Generate embeddings\n",
    "\n",
    "This is where the magic of semantic understanding happens.\n",
    "\n",
    "1. Load Model: We specify the name of a pre-trained model (all-MiniLM-L6-v2 is a popular choice – good balance of speed and accuracy) and load it using SentenceTransformer(MODEL_NAME). Downloading the model might take a moment the first time.\n",
    "2. `model.encode()`: We pass our list of text strings (corpus_texts) to the model's encode method. The model processes each string and converts it into a high-dimensional vector (a list of numbers, often 384 numbers long for this specific model) that represents its meaning. convert_to_numpy=True ensures the output is in a format (numpy arrays) suitable for the next step. show_progress_bar=True gives visual feedback as it can take time for large datasets.\n",
    "3. `corpus_embeddings`: The result is stored in `corpus_embeddings`, which is essentially a list where each item is an embedding vector corresponding to a text string in corpus_texts.\n",
    "4. `.astype('float32')`: We convert the embeddings to a specific numerical type (float32) because the FAISS library (our next step) requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6231a8a4-6859-48a6-8449-46a3ec6c7ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model 'all-MiniLM-L6-v2'...\n",
      "Model loaded in 1.90 seconds.\n",
      "Generating embeddings for the corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████| 22/22 [00:02<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated in 2.89 seconds.\n",
      "Embedding dimension: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2' # A good & fast general-purpose model\n",
    "\n",
    "# --- 2. Load Sentence Transformer Model ---\n",
    "print(f\"Loading Sentence Transformer model '{MODEL_NAME}'...\")\n",
    "start_time = time.time()\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "end_time = time.time()\n",
    "print(f\"Model loaded in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3. Generate Embeddings ---\n",
    "print(\"Generating embeddings for the corpus...\")\n",
    "start_time = time.time()\n",
    "# Ensure convert_to_numpy=True for FAISS compatibility\n",
    "corpus_embeddings = model.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "end_time = time.time()\n",
    "print(f\"Embeddings generated in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# FAISS requires float32 type\n",
    "corpus_embeddings = corpus_embeddings.astype('float32')\n",
    "\n",
    "# Get the dimensionality of embeddings (required by FAISS)\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4aac6-ebc9-4f56-9720-45cb15e8d2e0",
   "metadata": {},
   "source": [
    "# Build FAISS Index\n",
    "\n",
    "Now we organize our embeddings for efficient searching.\n",
    "\n",
    "1. `faiss.IndexFlatL2(embedding_dim)`: We create a simple FAISS index called IndexFlatL2. `embedding_dim` is the number of dimensions in our embeddings (e.g., 384, which we got from the shape of corpus_embeddings in the previous step). L2 refers to the distance metric (Euclidean distance) it will use to compare vectors – basically, how \"far apart\" their meanings are. \"Flat\" means it will perform an exhaustive search, comparing the query to every item, which is fine for smaller datasets but might be slow for millions of items (FAISS has more advanced index types for huge datasets).\n",
    "2. `index.add(corpus_embeddings)`: We add all our generated corpus_embeddings to the FAISS index. The index now stores these vectors in an optimized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f54afdb-8ba4-4f8c-8fe1-14d0022ff379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index (IndexFlatL2)...\n",
      "Adding 676 embeddings to the index...\n",
      "Index contains 676 vectors.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Build FAISS Index ---\n",
    "# Using IndexFlatL2 - simple baseline, performs exhaustive search\n",
    "# L2 distance = Euclidean distance\n",
    "print(\"Building FAISS index (IndexFlatL2)...\")\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# --- 5. Add Embeddings to Index ---\n",
    "print(f\"Adding {len(corpus_embeddings)} embeddings to the index...\")\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"Index contains {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c98df-93b1-4c1e-8dce-44afcbaf6f3d",
   "metadata": {},
   "source": [
    "# Query\n",
    "\n",
    "This is where we use the system we've built!\n",
    "\n",
    "1. `query_text`: Define the search query as a plain text string.\n",
    "2. Encode Query: Use the exact same loaded Sentence Transformer model (model.encode([query_text], ...)) to convert the query text into its own embedding vector. It needs to be in the same \"embedding space\" as the corpus embeddings for the comparison to be meaningful.\n",
    "3. `index.search()`: Use the FAISS index's search method. We provide the query embedding and the desired number of results (NUM_NEIGHBORS). FAISS quickly compares the query embedding to all the embeddings in the index and finds the NUM_NEIGHBORS closest ones based on the L2 distance. It returns two things: distances (how close each match is – lower is better) and indices (the positions/indices of the matches in the original list that we added to FAISS).\n",
    "4. Display Results:\n",
    "Loop through the returned indices.\n",
    "For each index idx returned by FAISS, use it to look up the corresponding original data in our corpus_metadata list (remember, we kept them in the same order!). This gives us back the full JSON object for the match.\n",
    "Print the rank (1st, 2nd, 3rd match), the distance score, and details from the retrieved JSON object.\n",
    "\n",
    "This final step shows the semantic search in action, retrieving the Nobel prize records that are most contextually relevant to the meaning of your search query, not just keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31e9352-23a4-4187-bc6e-78e8d0368a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing search for query: 'Breakthrough in medicine or physiology'\n",
      "Finding top 5 similar items...\n",
      "Query embedding generated in 0.01 seconds.\n",
      "Search completed in 0.0569 seconds.\n",
      "\n",
      "Search Results:\n",
      "--------------\n",
      "Rank 1:\n",
      "  Distance: 1.0166\n",
      "  ID: N/A\n",
      "  Category: medicine\n",
      "  Content: {'year': '1999', 'category': 'medicine', 'laureates': [{'id': '461', 'firstname': 'Günter', 'surname': 'Blobel', 'motivation': '\"for the discovery that proteins have intrinsic signals that govern their transport and localization in the cell\"', 'share': '1', 'full_name': 'Günter Blobel'}], 'jrag_text': 'Year: 1999 | Category: medicine | Laureats: Günter Blobel | Motivation: \"for the discovery that proteins have intrinsic signals that govern their transport and localization in the cell\"'}\n",
      "----------\n",
      "Rank 2:\n",
      "  Distance: 1.0256\n",
      "  ID: N/A\n",
      "  Category: medicine\n",
      "  Content: {'year': '2000', 'category': 'medicine', 'laureates': [{'id': '722', 'firstname': 'Arvid', 'surname': 'Carlsson', 'motivation': '\"for their discoveries concerning signal transduction in the nervous system\"', 'share': '3', 'full_name': 'Arvid Carlsson'}, {'id': '723', 'firstname': 'Paul', 'surname': 'Greengard', 'motivation': '\"for their discoveries concerning signal transduction in the nervous system\"', 'share': '3', 'full_name': 'Paul Greengard'}, {'id': '724', 'firstname': 'Eric', 'surname': 'Kandel', 'motivation': '\"for their discoveries concerning signal transduction in the nervous system\"', 'share': '3', 'full_name': 'Eric Kandel'}], 'jrag_text': 'Year: 2000 | Category: medicine | Laureats: [Arvid Carlsson, Paul Greengard, Eric Kandel] | Motivation: \"for their discoveries concerning signal transduction in the nervous system\"'}\n",
      "----------\n",
      "Rank 3:\n",
      "  Distance: 1.0405\n",
      "  ID: N/A\n",
      "  Category: medicine\n",
      "  Content: {'year': '2012', 'category': 'medicine', 'laureates': [{'id': '874', 'firstname': 'Sir John B.', 'surname': 'Gurdon', 'motivation': '\"for the discovery that mature cells can be reprogrammed to become pluripotent\"', 'share': '2', 'full_name': 'Sir John B. Gurdon'}, {'id': '875', 'firstname': 'Shinya', 'surname': 'Yamanaka', 'motivation': '\"for the discovery that mature cells can be reprogrammed to become pluripotent\"', 'share': '2', 'full_name': 'Shinya Yamanaka'}], 'jrag_text': 'Year: 2012 | Category: medicine | Laureats: [Sir John B. Gurdon, Shinya Yamanaka] | Motivation: \"for the discovery that mature cells can be reprogrammed to become pluripotent\"'}\n",
      "----------\n",
      "Rank 4:\n",
      "  Distance: 1.0466\n",
      "  ID: N/A\n",
      "  Category: medicine\n",
      "  Content: {'year': '1990', 'category': 'medicine', 'laureates': [{'id': '442', 'firstname': 'Joseph E.', 'surname': 'Murray', 'motivation': '\"for their discoveries concerning organ and cell transplantation in the treatment of human disease\"', 'share': '2', 'full_name': 'Joseph E. Murray'}, {'id': '443', 'firstname': 'E. Donnall', 'surname': 'Thomas', 'motivation': '\"for their discoveries concerning organ and cell transplantation in the treatment of human disease\"', 'share': '2', 'full_name': 'E. Donnall Thomas'}], 'jrag_text': 'Year: 1990 | Category: medicine | Laureats: [Joseph E. Murray, E. Donnall Thomas] | Motivation: \"for their discoveries concerning organ and cell transplantation in the treatment of human disease\"'}\n",
      "----------\n",
      "Rank 5:\n",
      "  Distance: 1.0475\n",
      "  ID: N/A\n",
      "  Category: medicine\n",
      "  Content: {'year': '1957', 'category': 'medicine', 'laureates': [{'id': '363', 'firstname': 'Daniel', 'surname': 'Bovet', 'motivation': '\"for his discoveries relating to synthetic compounds that inhibit the action of certain body substances, and especially their action on the vascular system and the skeletal muscles\"', 'share': '1', 'full_name': 'Daniel Bovet'}], 'jrag_text': 'Year: 1957 | Category: medicine | Laureats: Daniel Bovet | Motivation: \"for his discoveries relating to synthetic compounds that inhibit the action of certain body substances, and especially their action on the vascular system and the skeletal muscles\"'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Prepare and Perform Search ---\n",
    "query_text = \"Breakthrough in medicine or physiology\"\n",
    "NUM_NEIGHBORS = 5 # How many similar items to retrieve\n",
    "\n",
    "print(f\"\\nPerforming search for query: '{query_text}'\")\n",
    "print(f\"Finding top {NUM_NEIGHBORS} similar items...\")\n",
    "\n",
    "# Generate embedding for the query\n",
    "start_time = time.time()\n",
    "query_embedding = model.encode([query_text], convert_to_numpy=True)\n",
    "query_embedding = query_embedding.astype('float32')\n",
    "end_time = time.time()\n",
    "print(f\"Query embedding generated in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Perform the search\n",
    "start_time = time.time()\n",
    "# The search function returns distances and indices (IDs) of neighbors\n",
    "# query_embedding needs to be 2D array (even if it's just one query)\n",
    "distances, indices = index.search(query_embedding, NUM_NEIGHBORS)\n",
    "end_time = time.time()\n",
    "print(f\"Search completed in {end_time - start_time:.4f} seconds.\")\n",
    "\n",
    "# --- 7. Display Results ---\n",
    "print(\"\\nSearch Results:\")\n",
    "print(\"--------------\")\n",
    "\n",
    "# indices[0] contains the results for the first (and only) query\n",
    "# distances[0] contains the corresponding distances\n",
    "if not indices[0].size:\n",
    "    print(\"No results found.\")\n",
    "else:\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        # Map the index `idx` from FAISS back to our original data\n",
    "        # This works because we added embeddings in the same order as corpus_metadata\n",
    "        original_item_info = corpus_metadata[idx]\n",
    "        original_item = original_item_info['data']\n",
    "        distance = distances[0][i]\n",
    "\n",
    "        print(f\"Rank {i+1}:\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  ID: {original_item.get('id', 'N/A')}\")\n",
    "        print(f\"  Category: {original_item.get('category', 'N/A')}\")\n",
    "        print(f\"  Content: {original_item}\")\n",
    "        print(\"-\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jrag",
   "language": "python",
   "name": "jrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
