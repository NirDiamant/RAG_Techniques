{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Retrieval-Augmented Generation with the Trustworthy Language Model\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Trustworthy Retrieval-Augmented Generation\"/>\n",
    "  <meta property=\"og:title\" content=\"Trustworthy Retrieval-Augmented Generation\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Trustworthy Retrieval-Augmented Generation\" />\n",
    "  <meta name=\"image\" content=\"/img/rag.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/rag.png\" />\n",
    "  <meta name=\"description\" content=\"How to develop a reliable RAG system that quantifies the trustworthiness of every answer.\"  />\n",
    "  <meta property=\"og:description\" content=\"How to develop a reliable RAG system that quantifies the trustworthiness of every answer.\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to develop a reliable RAG system that quantifies the trustworthiness of every answer.\" />\n",
    "</head>\n",
    "\n",
    "This tutorial demonstrates how to replace the Generator LLM in any RAG system with Cleanlab's Trustworthy Language Model (TLM), to score the trustworthiness of answers and improve overall reliability.\n",
    "We recommend first completing the [TLM quickstart tutorial](https://help.cleanlab.ai/tutorials/tlm/).\n",
    "\n",
    "A second part of this tutorial demonstrates how to alternatively use TLM in an existing RAG pipeline where low-latency is key.\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** has become popular for building LLM-based Question-Answer systems in domains where LLMs alone suffer from: hallucination, knowledge gaps, and factual inaccuracies. However, RAG systems often still produce unreliable responses, because they depend on LLMs that are fundamentally unreliable. Cleanlab's Trustworthy Language Model (TLM) offers a solution by providing trustworthiness scores to assess and improve response quality, **independent of your RAG architecture or retrieval and indexing processes**. To diagnose when RAG answers cannot be trusted, simply swap your existing LLM that is generating answers based on the retrieved context with TLM. This tutorial showcases this for a standard RAG system, based off a tutorial in the popular [LlamaIndex](https://docs.llamaindex.ai/) framework. Here we merely replace the LLM used in the LlamaIndex tutorial with TLM, and showcase some of the benefits. TLM can be similarly inserted into *any* other RAG framework.\n",
    "\n",
    "![TLM RAG system correctly identifying high/low confidence responses](../images/tlm_rag_thumbnail.png)\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "RAG is all about connecting LLMs to data, to better inform their answers. This tutorial uses Nvidia's Q1 FY2024 earnings report as an example dataset.\n",
    "Use the following commands to download the data (earnings report) and store it in a directory named `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/NVIDIA_Financial_Results_Q1_FY2024.md'\n",
    "!mkdir -p ./data\n",
    "!mv NVIDIA_Financial_Results_Q1_FY2024.md data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's next install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U cleanlab-studio llama-index llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our Cleanlab client.\n",
    "You can get your Cleanlab API key here: https://app.cleanlab.ai/account after creating an account. For detailed instructions, refer to [this guide](https://help.cleanlab.ai/guide/quickstart/api/#api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio(\"<insert your API key>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate TLM with LlamaIndex\n",
    "\n",
    "\n",
    "TLM not only provides a response but also includes a **trustworthiness score** indicating the confidence that this response is good/accurate.\n",
    "Here we initialize a TLM object with default settings. You can achieve better results by playing with the TLM configurations outlined in the Advanced section of the [TLM quickstart tutorial](https://help.cleanlab.ai/tutorials/tlm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG pipeline closely follows the LlamaIndex guide on [Using a custom LLM Model](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/#example-using-a-custom-llm-model-advanced). LLamaIndex's `CustomLLM` class exposes two methods, `complete()` and `stream_complete()`, for returning the LLM response. Additionally, it provides a `metadata` property to specify LLM details such as context window, number of output tokens, and name of your LLM.\n",
    "\n",
    "Here we create a `TLMWrapper` subclass of `CustomLLM` that uses our TLM object instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import json\n",
    "\n",
    "# Import LlamaIndex dependencies\n",
    "from llama_index.core.base.llms.types import (\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "\n",
    "class TLMWrapper(CustomLLM):\n",
    "    context_window: int = 16000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"TLM\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response: Dict[str, str] = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "        return CompletionResponse(text=output)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "\n",
    "        # Stream the output\n",
    "        output_str = \"\"\n",
    "        for token in output:\n",
    "            output_str += token\n",
    "            yield CompletionResponse(text=output_str, delta=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RAG pipeline with TLM\n",
    "\n",
    "Now let's integrate our TLM-based `CustomLLM` into a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = TLMWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Embedding Model\n",
    "\n",
    "RAG uses an embedding model to match queries against document chunks to retrieve the most relevant data. Here we opt for a no-cost, local embedding model from Hugging Face. You can use any other embedding model by referring to this [LlamaIndex guide](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Create Index + Query Engine\n",
    "\n",
    "Let's create an index from the documents stored in the data directory. The system can index multiple files within the same folder, although for this tutorial, we'll use just one document.\n",
    "We stick with the default index from LlamaIndex for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "# Optional step since we're loading just one data file\n",
    "for doc in documents: \n",
    "    doc.excluded_llm_metadata_keys.append(\"file_path\") # file_path wouldn't be a useful metadata to add to LLM's context since our datasource contains just 1 file\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated index is used to power a query engine over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TLM is agnostic to the index and the query engine used for RAG, and is compatible with any choices you make for these components of your system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering queries with our RAG system\n",
    "\n",
    "Let's try out our RAG pipeline based on TLM. Here we pose questions with differing levels of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Define `display_response` helper function\n",
    "\n",
    "# This method presents formatted responses from our TLM-based RAG pipeline. It parses the output to display both the response itself and the corresponding trustworthiness score.\n",
    "def display_response(response):\n",
    "    response_str = response.response\n",
    "    output_dict = json.loads(response_str)\n",
    "    print(f\"Response: {output_dict['response']}\")\n",
    "    print(f\"Trustworthiness score: {round(output_dict['trustworthiness_score'], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Questions\n",
    "\n",
    "We first pose straightforward questions that can be directly answered by the provided data and can be easily located within a few lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: NVIDIA's total revenue in the first quarter of fiscal 2024 was $7.19 billion.\n",
      "Trustworthiness score: 1.0\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was NVIDIA's total revenue in the first quarter of fiscal 2024?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The GAAP earnings per diluted share for the quarter (Q1 FY24) was $0.82.\n",
      "Trustworthiness score: 0.99\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was the GAAP earnings per diluted share for the quarter?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Jensen Huang, NVIDIA's CEO, commented on the significant transitions the computer industry is undergoing, particularly in the areas of accelerated computing and generative AI.\n",
      "Trustworthiness score: 0.99\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What significant transitions did Jensen Huang, NVIDIA's CEO, comment on?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM returns high trustworthiness scores for these responses, indicating high confidence they are accurate. After doing a quick fact-check (reviewing the original earnings report), we can confirm that TLM indeed accurately answered these questions. In case you're curious, here are relevant excerpts from the data context for these questions:\n",
    "\n",
    "> NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, ...\n",
    "\n",
    "> GAAP earnings per diluted share for the quarter were $0.82, up 28% from a year ago and up 44% from the previous quarter.\n",
    "\n",
    "> Jensen Huang, founder and CEO of NVIDIA, commented on the significant transitions the computer industry is undergoing, particularly accelerated computing and generative AI, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions without Available Context \n",
    "\n",
    "Now let's see how TLM responds to queries that *cannot* be answered using the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report indicates that NVIDIA's professional visualization (proviz) revenue declined by 53% from a year ago. While the specific factors contributing to this decline are not detailed in the provided context, it can be inferred that broader market trends, such as shifts in demand for professional visualization products, competition, or changes in customer spending patterns, may have played a role. Additionally, the overall economic environment and the impact of the ongoing transitions in the computer industry, particularly in areas like accelerated computing and generative AI, could also have influenced the decline in proviz revenue.\n",
      "Trustworthiness score: 0.76\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What factors as per the report were responsible to the decline in NVIDIA's proviz revenue?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower TLM trustworthiness score indicate a bit more uncertainty about the response, which aligns with the lack of information available. Let's try some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report indicates that NVIDIA's Gaming revenue decreased year over year by 38%, but it does not provide specific reasons for this decline. However, it does highlight several developments in the gaming segment, such as the launch of new GPUs and the expansion of game titles, which suggest that while there may be challenges in the market, NVIDIA is actively working to enhance its product offerings and gaming ecosystem. The decrease in revenue could be attributed to broader market conditions, reduced consumer spending, or increased competition, but these factors are not explicitly mentioned in the provided context.\n",
      "Trustworthiness score: 0.92\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does the report explain why NVIDIA's Gaming revenue decreased year over year?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The context information provided does not include specific details about the industry average for dividend payouts. Therefore, I cannot directly compare NVIDIA's dividend payout for this quarter to the industry average. However, NVIDIA announced a quarterly cash dividend of $0.04 per share for shareholders of record on June 8, 2023. To assess how this compares to the industry average, one would need to look up the average dividend payout for similar companies in the technology or semiconductor industry.\n",
      "Trustworthiness score: 0.93\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does NVIDIA's dividend payout for this quarter compare to the industry average?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that TLM demonstrates the ability to recognize the limitations of the available information. It refrains from generating speculative responses or hallucinations, thereby maintaining the reliability of the question-answering system. This behavior showcases an understanding of the boundaries of the context and prioritizes accuracy over conjecture. \n",
    "\n",
    "### Challenging Questions\n",
    "\n",
    "Let's see how our RAG system responds to harder questions, some of which may be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: NVIDIA's revenue for the first quarter ended April 30, 2023, was $7.19 billion. The revenue for the previous quarter (the fourth quarter of fiscal 2023) can be calculated by taking the reported revenue for the first quarter and adjusting for the 19% increase from the previous quarter. \n",
      "\n",
      "Let \\( x \\) be the revenue for the previous quarter. According to the information provided:\n",
      "\n",
      "\\[\n",
      "x + 0.19x = 7.19 \\text{ billion}\n",
      "\\]\n",
      "\\[\n",
      "1.19x = 7.19 \\text{ billion}\n",
      "\\]\n",
      "\\[\n",
      "x = \\frac{7.19 \\text{ billion}}{1.19} \\approx 6.04 \\text{ billion}\n",
      "\\]\n",
      "\n",
      "Now, to find the decrease in revenue from the previous quarter to this quarter:\n",
      "\n",
      "\\[\n",
      "\\text{Decrease} = 7.19 \\text{ billion} - 6.04 \\text{ billion} \\approx 1.15 \\text{ billion}\n",
      "\\]\n",
      "\n",
      "Thus, NVIDIA's revenue decreased by approximately $1.15 billion this quarter compared to the last quarter.\n",
      "Trustworthiness score: 0.6\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How much did Nvidia's revenue decrease this quarter vs last quarter, in dollars?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report mentions the following companies: Microsoft. There are no mentions of Dell or ServiceNow in the provided context information.\n",
      "Trustworthiness score: 0.73\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"This report focuses on Nvidia's Q1FY2024 financial results. There are mentions of other companies in the report like Microsoft, Dell, ServiceNow, etc. Can you name them all here?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: In NVIDIA's Q1 FY2024 financial results, the following RTX GPU models were officially announced:\n",
      "\n",
      "1. **GeForce RTX 4060 family of GPUs**\n",
      "2. **GeForce RTX 4070 GPU**\n",
      "3. **Six new NVIDIA RTX GPUs for mobile and desktop workstations**\n",
      "\n",
      "This totals to **eight RTX GPU models** announced.\n",
      "Trustworthiness score: 0.74\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How many RTX GPU models, including all custom versions released by third-party manufacturers and all revisions across different series, were officially announced in NVIDIA's Q1 FY2024 financial results?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: To calculate the projected annual revenue for NVIDIA's Data Center segment if it maintains its Q1 FY2024 quarter-over-quarter growth rate, we first need to determine the growth rate from Q4 FY2023 to Q1 FY2024.\n",
      "\n",
      "NVIDIA reported a record Data Center revenue of $4.28 billion for Q1 FY2024. The revenue for the previous quarter (Q4 FY2023) can be calculated as follows:\n",
      "\n",
      "Let \\( R \\) be the revenue for Q4 FY2023. The growth rate from Q4 FY2023 to Q1 FY2024 is given by:\n",
      "\n",
      "\\[\n",
      "\\text{Growth Rate} = \\frac{\\text{Q1 Revenue} - \\text{Q4 Revenue}}{\\text{Q4 Revenue}} = \\frac{4.28 - R}{R}\n",
      "\\]\n",
      "\n",
      "We know that the overall revenue for Q1 FY2024 is $7.19 billion, which is up 19% from the previous quarter. Therefore, we can express the revenue for Q4 FY2023 as:\n",
      "\n",
      "\\[\n",
      "\\text{Q1 FY2024 Revenue} = \\text{Q4 FY2023 Revenue} \\times 1.19\n",
      "\\]\n",
      "\n",
      "Substituting the known value:\n",
      "\n",
      "\\[\n",
      "7.19 = R \\times 1.19\n",
      "\\]\n",
      "\n",
      "Solving for \\( R \\):\n",
      "\n",
      "\\[\n",
      "R = \\frac{7.19}{1.19} \\approx 6.03 \\text{ billion}\n",
      "\\]\n",
      "\n",
      "Now, we can calculate the Data Center revenue for Q4 FY2023. Since we don't have the exact figure for the Data Center revenue in Q4 FY2023, we will assume that the Data Center revenue also grew by the same percentage as the overall revenue. \n",
      "\n",
      "Now, we can calculate the quarter-over-quarter growth rate for the Data Center segment:\n",
      "\n",
      "\\[\n",
      "\\text{Growth Rate} = \\frac{4.28 - R_D}{R_D}\n",
      "\\]\n",
      "\n",
      "Where \\( R_D \\) is the Data Center revenue for Q4 FY2023. However, we need to find \\( R_D \\) first. \n",
      "\n",
      "Assuming the Data Center revenue was a certain percentage of the total revenue in Q4 FY2023, we can estimate it. For simplicity, let's assume the Data Center revenue was around 50% of the total revenue in Q4 FY2023 (this is a rough estimate, as we don't have the exact figure).\n",
      "\n",
      "Thus, \\( R_D \\approx 0.5 \\times 6\n",
      "Trustworthiness score: 0.69\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"If NVIDIA's Data Center segment maintains its Q1 FY2024 quarter-over-quarter growth rate for the next four quarters, what would be its projected annual revenue?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM automatically alerts us that these answers are unreliable, by the low trustworthiness score. RAG systems with TLM help you properly exercise caution when you see low trustworthiness scores. Here are the correct answers to the aforementioned questions:\n",
    "\n",
    "> NVIDIA's revenue increased by $1.14 billion this quarter compared to last quarter.\n",
    "\n",
    "> Google, Amazon Web Services, Microsoft, Oracle, ServiceNow, Medtronic, Dell Technologies.\n",
    "\n",
    "> There is not a specific total count of RTX GPUs mentioned.\n",
    "\n",
    "> Projected annual revenue if this growth rate is maintained for the next four quarters: approximately $26.34 billion.\n",
    "\n",
    "With TLM, you can easily increase trust in any RAG system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate low-latency/streaming approach: Use TLM to assess responses from an existing RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsequent part of the tutorial demonstrates how to use TLM to assess RAG responses from any other generator LLM in a streaming use case (rather than producing the responses with TLM too).\n",
    "TLM can score the trustworthiness of *any* response (generated by another LLM model) to a given prompt via [TLM.get_trustworthiness_score_async()](https://help.cleanlab.ai/reference/python/trustworthy_language_model/#method-get_trustworthiness_score_async).\n",
    "This is useful in settings where low latency is critical, since you can first stream in the response from your existing RAG system, and subsequently stream in TLM's trustworthiness score once it has been computed.\n",
    "\n",
    "A key consideration here is the `prompt` argument provided to TLM. In order for TLM to effectively detect bad responses and hallucinations, its provided `prompt` should contain all relevant information, including:\n",
    "- Optional **system instructions** to shape the generator LLM's overall behavior\n",
    "- Optional **criteria** that a good response should satisfy\n",
    "- Relevant **context** fetched by the RAG system retriever (in the same format as provided to your generator LLM)\n",
    "- The **user query** being responded to.\n",
    "\n",
    "\n",
    "Here we demonstrate this process assuming a RAG system that uses the `gpt-4o-mini` LLM from OpenAI, but this can be done with any LLM.\n",
    "First, let's set up the OpenAI client for streaming responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"<insert your OpenAI API key>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function for streaming a response from OpenAI and then scoring its trustworthiness via TLM.\n",
    "\n",
    "In your RAG system, the retriever will fetch some context for your generator LLM. You should combine that context with the user query into the `prompt` argument of `TLM.get_trustworthiness_score_async()`.\n",
    "If your RAG system uses system instructions to shape the generator LLM's overall behavior, ensure these are also part of the `prompt` argument passed to TLM. If you have criteria that a good response should satisfy (eg. conciseness, specific statements to avoid, etc), also provide these as part of the `prompt` argument passed to TLM (ideally formulated as: `A correct answer will meet the following criteria: ...`).\n",
    "\n",
    "If the `prompt` argument provided to TLM lacks any of the above information, then the resulting trustworthiness score may be lacking.\n",
    "\n",
    "We provide an example RAG prompt you can use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_openai_response(\n",
    "    query: str,\n",
    "    context: str,\n",
    "    system_instructions: str = None,\n",
    "    eval_criteria: str = None\n",
    ") -> AsyncGenerator[str, None]:\n",
    "    \"\"\"\n",
    "    Asynchronously stream a response from OpenAI, and subsequently provide a trustworthiness score using TLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        context (str): Retrieved/formatted context information to be used for answering the query.\n",
    "        system_instructions (str): Optional instructions for the LLM on how to behave overall.\n",
    "        eval_criteria (str): Optional criteria for evaluating the correctness/quality of the answer.\n",
    "\n",
    "    Yields:\n",
    "        str: Chunks of the streamed response from OpenAI, followed by the trustworthiness score.\n",
    "\n",
    "    Note:\n",
    "        The function yields the response in chunks for a streaming effect, and the trustworthiness\n",
    "        score is yielded at the end after the full response is received.\n",
    "    \"\"\"\n",
    "    full_prompt = f\"\"\"{system_instructions}\n",
    "\n",
    "    {eval_criteria}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User Question: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    response_stream = await client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    async for chunk in response_stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            yield content\n",
    "\n",
    "    # After response streaming is complete, get the trustworthiness score for this prompt/response pair.\n",
    "    # Here we demonstrate the asynchronous method in case there is additional logic you'd like to execute while the trustworthiness score is being computed.\n",
    "    trust_score_result = await tlm.get_trustworthiness_score_async(full_prompt, full_response)\n",
    "    yield f\"\\n\\nTrustworthiness Score: {trust_score_result['trustworthiness_score']:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we suppose the context for your generator LLM has already been fetched by the retriever of your RAG system. The context used below includes the relevant excerpts about Nvidia from a document encountered earlier in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context and query for the OpenAI streaming response\n",
    "context = \"\"\" # Provided by your RAG system retriever \n",
    "NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, up 19% from the previous quarter and down 13% from a year ago.\n",
    "\n",
    "GAAP earnings per diluted share for the quarter were $0.82, up 28% from a year ago and up 44% from the previous quarter. Non-GAAP earnings per diluted share were $1.09, down 20% from a year ago and up 26% from the previous quarter.\n",
    "\n",
    "\"The computer industry is going through two simultaneous transitions -- accelerated computing and generative AI,\" said Jensen Huang, founder and CEO of NVIDIA. \"A trillion dollars of installed global data center infrastructure will transition from general purpose to accelerated computing as companies race to apply generative AI into every product, service and business process.\n",
    "\"\"\"\n",
    "\n",
    "query = \"What was NVIDIA's revenue in the first quarter of 2023, and how does it compare to the previous quarter? And what are the two simultaneous transitions Jensen Huang mentioned?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define system instructions and evaluation criteria. These are optional, but if you are implementing them in your RAG system, try to also provide them to TLM.\n",
    "\n",
    "\n",
    "**System instructions for the LLM:**\n",
    "- Note: Edit these instructions to match the system instructions used for the generator LLM in your own RAG system.\n",
    "- These instructions should guide the LLM on how to interpret and respond to queries based on the provided context.\n",
    "\n",
    "**Evaluation criteria for the response:**\n",
    "- Note: Edit these criteria to reflect the specific requirements for a good response in your use case.\n",
    "- These criteria will be used by TLM to assess the trustworthiness of the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System instructions for the LLM\n",
    "system_instructions = \"\"\"You are a helpful assistant designed to help users navigate a complex set of documents. Answer the user's Question based on the following Context. Follow these rules:\n",
    "1. Use only information from the provided Context.\n",
    "2. If the Context doesn't adequately address the Question, say: \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "3. Give a clear, short, and accurate answer. Explain complex terms if needed.\n",
    "4. If the Context contains conflicting information, point this out without attempting to resolve the conflict.\n",
    "5. Don't use phrases like \"according to the context,\" \"as the context states,\" etc.\n",
    "Remember, your purpose is to provide information based on the Context, not to offer original advice.\"\"\"\n",
    "\n",
    "# Evaluation criteria for the response\n",
    "eval_criteria = \"\"\"A correct Answer should: be concise without unnecessary words, only contain facts that are explicitly stated in the provided Context, and never contain investment advice.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually stream in a RAG response and trustworthiness score using our `stream_openai_response()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "NVIDIA's revenue for the first quarter of 2023 was $7.19 billion, which is up 19% from the previous quarter. Jensen Huang mentioned that the computer industry is going through two simultaneous transitions: accelerated computing and generative AI.\n",
      "\n",
      "Trustworthiness Score: 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Streaming response:\")\n",
    "async for content in stream_openai_response(query, context, system_instructions, eval_criteria):\n",
    "    print(content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the above code, you'll see the OpenAI response streaming in real-time, followed by the trustworthiness score from TLM. This approach gives you the best of both worlds: a responsive interface with low-latency streaming and a reliability assessment of the generated answer to catch hallucinations in your RAG system.\n",
    "\n",
    "If you have an existing RAG pipeline where latency is a key concern, this is how we recommend incorporating TLM to catch hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate TLM scores to more closely reflect response-quality ratings by your team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If TLM trustworthiness scores do not align with your team's manual quality ratings of good/bad RAG responses, consider introducing *custom evaluation criteria*. You can further calibrate TLM scores against your human quality ratings for a dataset of prompt-response pairs.\n",
    "\n",
    "For example, let's define custom evaluation criteria based on *faithfulness* and *groundedness*, two key measures of RAG systems. Faithfulness measures the factual consistency of the generated answer against the retrieved context, while groundedness measures whether information in the generated answer is grounded in the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_groundedness_eval_criteria = {\n",
    "    \"custom_eval_criteria\": [\n",
    "        {\n",
    "            \"name\": \"Faithfulness & Groundedness\",\n",
    "            \"criteria\": \"Determine if the Answer is solely based on information available in the Context (no additional facts are mentioned in the Answer that are not stated in the Context). \\\n",
    "                        Also determine if the Answer does not contradict any information in the Context. If the Context contains no information available to answer the Question, a good Answer should state 'there is no information available.'\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this custom evaluation criteria, there are 4 cases we want to consider:\n",
    "\n",
    "1. Question is answerable based on context, response answers the question - (should be a high score)\n",
    "2. Question is answerable based on context, response does not answer the question - (should be a low score)\n",
    "3. Question is not answerable based on context, response answers the question - (should be a low score)\n",
    "4. Question is not answerable based on context, response does not answer the question - (should be a high score)\n",
    "\n",
    "Let's now define some example data based on our Nvidia dataset we've used throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "custom_eval_data = [\n",
    "    {\"question\": \"What was NVIDIA's revenue in the first quarter of fiscal 2024?\", \n",
    "     \"context\": \"NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion.\", \n",
    "     \"answer\": \"NVIDIA's total revenue in the first quarter of fiscal 2024 was $7.19 billion.\", \n",
    "     \"case\": 1},\n",
    "    \n",
    "    {\"question\": \"What was NVIDIA's revenue in the first quarter of fiscal 2024?\", \n",
    "     \"context\": \"NVIDIA's GAAP earnings per share for the quarter were $0.82.\", \n",
    "     \"answer\": \"NVIDIA's total revenue was $7.19 billion.\", \n",
    "     \"case\": 2},\n",
    "    \n",
    "    {\"question\": \"What was NVIDIA's dividend payout in Q1 FY2024?\", \n",
    "     \"context\": \"NVIDIA reported earnings per share, but no mention of dividends.\", \n",
    "     \"answer\": \"NVIDIA's dividend payout was $0.10 per share.\", \n",
    "     \"case\": 3},\n",
    "    \n",
    "    {\"question\": \"What was NVIDIA's dividend payout in Q1 FY2024?\", \n",
    "     \"context\": \"NVIDIA reported earnings per share, but no mention of dividends.\", \n",
    "     \"answer\": \"No information is available to answer the question.\", \n",
    "     \"case\": 4}\n",
    "]\n",
    "\n",
    "custom_df = pd.DataFrame(custom_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the prompt used for this custom evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tlm_prompt(row):\n",
    "    return f\"Context: {row['context']}\\n\\nUser Question: {row['question']}\"\n",
    "\n",
    "custom_df['prompt'] = custom_df.apply(create_tlm_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_faithfulness_groundedness = studio.TLM(options=faithfulness_groundedness_eval_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use [TLM.get_trustworthiness_score()](https://help.cleanlab.ai/reference/python/trustworthy_language_model/#method-get_trustworthiness_score) to obtain the score pertaining to our faithfulness & groundedness custom evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|██████████|\n"
     ]
    }
   ],
   "source": [
    "res_faithfulness_groundedness = tlm_faithfulness_groundedness.get_trustworthiness_score(custom_df['prompt'].tolist(), custom_df['answer'].tolist())\n",
    "res_faithfulness_groundedness_df = pd.DataFrame(res_faithfulness_groundedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was NVIDIA's revenue in the first quarter...</td>\n",
       "      <td>NVIDIA's total revenue in the first quarter of...</td>\n",
       "      <td>0.986734</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Faithfulne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was NVIDIA's revenue in the first quarter...</td>\n",
       "      <td>NVIDIA's total revenue was $7.19 billion.</td>\n",
       "      <td>0.775447</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Faithfulne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was NVIDIA's dividend payout in Q1 FY2024?</td>\n",
       "      <td>NVIDIA's dividend payout was $0.10 per share.</td>\n",
       "      <td>0.565589</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Faithfulne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was NVIDIA's dividend payout in Q1 FY2024?</td>\n",
       "      <td>No information is available to answer the ques...</td>\n",
       "      <td>0.418083</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Faithfulne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What was NVIDIA's revenue in the first quarter...   \n",
       "1  What was NVIDIA's revenue in the first quarter...   \n",
       "2    What was NVIDIA's dividend payout in Q1 FY2024?   \n",
       "3    What was NVIDIA's dividend payout in Q1 FY2024?   \n",
       "\n",
       "                                              answer  trustworthiness_score  \\\n",
       "0  NVIDIA's total revenue in the first quarter of...               0.986734   \n",
       "1          NVIDIA's total revenue was $7.19 billion.               0.775447   \n",
       "2      NVIDIA's dividend payout was $0.10 per share.               0.565589   \n",
       "3  No information is available to answer the ques...               0.418083   \n",
       "\n",
       "                                                 log  \n",
       "0  {'custom_eval_criteria': [{'name': 'Faithfulne...  \n",
       "1  {'custom_eval_criteria': [{'name': 'Faithfulne...  \n",
       "2  {'custom_eval_criteria': [{'name': 'Faithfulne...  \n",
       "3  {'custom_eval_criteria': [{'name': 'Faithfulne...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.concat([custom_df, res_faithfulness_groundedness_df], axis=1)\n",
    "df_results[['question', 'answer', 'trustworthiness_score', 'log']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our custom evaluation scores align with expected behavior from each of the previously-described cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 - Faithfulness & Groundedness Score: 1.00\n",
      "Case 2 - Faithfulness & Groundedness Score: 0.20\n",
      "Case 3 - Faithfulness & Groundedness Score: 0.20\n",
      "Case 4 - Faithfulness & Groundedness Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_results.iterrows():\n",
    "    case = row['case']\n",
    "    faithfulness_and_groundedness_score = row['log']['custom_eval_criteria'][0]['score']\n",
    "    print(f\"Case {case} - Faithfulness & Groundedness Score: {faithfulness_and_groundedness_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected cases 1 and 4 to have a high score and cases 2 and 3 to have a low score, so the results look great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have human-quality ratings for many generated responses and want to produce automated scores that better align with these ratings, consider [TLMCalibrated](https://help.cleanlab.ai/tutorials/tlm_custom_eval/#calibrating-tlm-scores-against-human-quality-ratings). This approach combines TLM's trustworthiness and custom evaluation scores into a single score that is calibrated to match your manually-provided quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality ratings for each response, say provided by your team (1 = good, 0 = bad)\n",
    "df_results['human_rating'] = [1, 0, 0, 1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_calibrated = studio.TLMCalibrated(options=faithfulness_groundedness_eval_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the `TLMCalibrated` model to a dataset of the previously-obtained TLM scores and human quality ratings, training the model to better align its scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_calibrated.fit(res_faithfulness_groundedness_df.to_dict(orient='records'), df_results['human_rating'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to produce calibrated scores after fitting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|██████████|\n"
     ]
    }
   ],
   "source": [
    "calibrated_res = tlm_calibrated.get_trustworthiness_score(custom_df['prompt'].tolist(), custom_df['answer'].tolist())\n",
    "calibrated_res_df = pd.DataFrame(calibrated_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was NVIDIA's revenue in the first quarter...</td>\n",
       "      <td>NVIDIA's total revenue in the first quarter of...</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was NVIDIA's revenue in the first quarter...</td>\n",
       "      <td>NVIDIA's total revenue was $7.19 billion.</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was NVIDIA's dividend payout in Q1 FY2024?</td>\n",
       "      <td>NVIDIA's dividend payout was $0.10 per share.</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was NVIDIA's dividend payout in Q1 FY2024?</td>\n",
       "      <td>No information is available to answer the ques...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What was NVIDIA's revenue in the first quarter...   \n",
       "1  What was NVIDIA's revenue in the first quarter...   \n",
       "2    What was NVIDIA's dividend payout in Q1 FY2024?   \n",
       "3    What was NVIDIA's dividend payout in Q1 FY2024?   \n",
       "\n",
       "                                              answer  calibrated_score  \n",
       "0  NVIDIA's total revenue in the first quarter of...              0.97  \n",
       "1          NVIDIA's total revenue was $7.19 billion.              0.03  \n",
       "2      NVIDIA's dividend payout was $0.10 per share.              0.03  \n",
       "3  No information is available to answer the ques...              0.81  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_combined_df = pd.concat([df_results, calibrated_res_df], axis=1)\n",
    "calibrated_combined_df[['question', 'answer', 'calibrated_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores even better align with our expectations (high scores for question/response pairs 1 and 4, low scores for pairs 2 and 3).\n",
    "\n",
    "Learn more in our [TLM Custom Evaluation Criteria tutorial](https://help.cleanlab.ai/tutorials/tlm_custom_eval/#calibrating-tlm-scores-against-human-quality-ratings)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
