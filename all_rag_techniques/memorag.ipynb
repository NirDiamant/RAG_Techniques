{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MemoRAG: Enhancing Retrieval-Augmented Generation with Memory Models\n",
    "\n",
    "## Overview\n",
    "MemoRAG is a Retrieval-Augmented Generation (RAG) framework that incorporates a memory model as an auxiliary step before the retrieval phase. In doing so, it bridges the gap in contextual understanding and reasoning that standard RAG techniques face when addressing queries with implicit or ambiguous information needs and unstructured external knowledge.\n",
    "\n",
    "## Motivation\n",
    "Standard RAG techniques rely heavily on lexical or semantic matching between the query and the knowledge base. While this approach works well for clear question answering tasks with structured knowledge, it often falls short when handling queries with implicit or ambiguous information (e.g., describing the relationships between main characters in a novel) or when the knowledge base is unstructured (e.g., fiction books). In such cases, lexical or semantic matching seldom produces the desired outputs.\n",
    "\n",
    "## Key Components\n",
    "1. **Memory**: A compressed representation of the database created by a long-context model, designed to handle and summarize extensive inputs efficiently.\n",
    "2. **Retriever**: A standard RAG retrieval model responsible for selecting relevant context from the knowledge base to support the generator.\n",
    "3. **Generator**: A generative language model that produces responses by combining the query with the retrieved context, similar to standard RAG setups.\n",
    "\n",
    "## Method Details\n",
    "### 1. Memory\n",
    "- The memory module serves as an auxiliary component to enhance the retriever’s ability to identify better matches between queries and relevant parts of the database. It takes the original query and the database as inputs and produces staging answers — intermediate outputs like clues, surrogate queries, or key points — which the retriever uses instead of the original query.\n",
    "- Long-term memory is constructed by running a long-context model, such as Qwen2-7B-Instruct or Mistral-7B-Instruct-v0.2, over the entire database. This process generates a compressed representation of the database through an attention mechanism.\n",
    "- The compressed representation is stored as key-value pairs, facilitating efficient and accurate retrieval.\n",
    "- Released memory models include memorag-qwen2-7b-inst and memorag-mistral-7b-inst, derived from Qwen2-7B-Instruct and Mistral-7B-Instruct-v0.2, respectively.\n",
    "\n",
    "### 2. Retriever\n",
    "- The retriever is a standard retrieval model, adapted to take processed queries (created by the memory module as staging answers) instead of the original query.\n",
    "- It outputs the retrieved **context**, which serves as the basis for generating the final answer.\n",
    "\n",
    "\n",
    "### 3. Generator\n",
    "- The generator produces the final response by combining the retriever’s output (retrieved context) with the original query.\n",
    "- MemoRAG ensures compatibility and consistency by using the memory module’s underlying model as the default generator.\n",
    "\n",
    "## Benefits of the Approach\n",
    "1. **Extended Scope of Queries:** MemoRAG's preprocessing capabilities enable it to handle complex and long-context tasks that conventional RAG methods struggle with.\n",
    "\n",
    "2. **Improved Accuracy:** By simplifying and adjusting queries before retrieval, MemoRAG enhances performance over standard RAG methods.\n",
    "\n",
    "3. **Flexibility:** Adapts to diverse tasks, datasets, and retrieval scenarios.\n",
    "\n",
    "4. **Robustness:** Improved performance remains consistent across various generators, datasets, and query types.\n",
    "\n",
    "5. **Efficiency**: The use of key-value compression reduces computational overhead.\n",
    "\n",
    "## Conclusion\n",
    "The memory module in MemoRAG significantly enhances comprehension of both the queries and the database, enabling more effective retrieval. Its ability to preprocess queries, generate staging answers, and leverage long-context memory models ensures high-quality responses, making MemoRAG a significant step forward in the evolution of retrieval-augmented generation.\n"
   ],
   "id": "67e98dec56de7fdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/memo_rag.svg\" alt=\"MemoRAG\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ],
   "id": "abfa998b6aa7d7ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementation",
   "id": "76c252dd34083769"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "2bc3b3e34c811bd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:09.261956Z",
     "start_time": "2025-03-15T14:59:08.521581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "from helper_functions import *\n"
   ],
   "id": "e31a95191a274c83",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAI Setup",
   "id": "a84d4f5a1fd94512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:09.493939Z",
     "start_time": "2025-03-15T14:59:09.480567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ],
   "id": "705080e6703ad71c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory Module Classes",
   "id": "62fd38f51251d71a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:11.723022Z",
     "start_time": "2025-03-15T14:59:11.720060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KeyValuePair(BaseModel):\n",
    "    topic: str\n",
    "    details: str\n",
    "\n",
    "class MemoryResponse(BaseModel):\n",
    "    pairs: List[KeyValuePair]"
   ],
   "id": "c3ec6a3e2153dc74",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:12.099243Z",
     "start_time": "2025-03-15T14:59:12.088406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MemoryStore:\n",
    "    \"\"\"The MemoryStore class is a realization of the Memory Module discussed in the paper.\n",
    "    Its 'memorize' method is used to create a prompt cache mimicking a key-value compression of the original text (database).\n",
    "    This cache can then be used by the 'create_retrieval_prompt' method for creating the processed query to be used later for retrieval\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.store = None\n",
    "        self.processed_count = 0\n",
    "        self.json_parse_failures = 0\n",
    "        self._last_parse_used_fallback = False\n",
    "\n",
    "\n",
    "    def memorize(self, document: str):\n",
    "        \"\"\"Process document into key-value pairs and store them\"\"\"\n",
    "\n",
    "        # self.reset()\n",
    "        # batch_size = 10\n",
    "        system_prompt = (\n",
    "            \"You are an expert at extracting structured key-value pairs from text. \"\n",
    "            \"Identify key topics, entities, or questions and pair them with relevant, detailed information. \"\n",
    "            \"Ensure the output is well-structured, avoiding redundancy while preserving meaning.\"\n",
    "        )\n",
    "\n",
    "        kv_cache_prompt = \"\"\"\n",
    "        You are provided with a long article, chunk by chunk. Read each chunk carefully and extract key topics and their detailed information.\n",
    "        For each key topic:\n",
    "        1. Identify the main concept, entity, or potential question\n",
    "        2. Provide the corresponding detailed information or answer\n",
    "\n",
    "        Note: the aim is to mimic kv cache memory creation\n",
    "\n",
    "        Now, the article begins:\n",
    "        {document}\n",
    "\n",
    "        The article ends here.\n",
    "\n",
    "        RESPONSE FORMAT IS IMPORTANT. You must return a JSON object with the following structure:\n",
    "        {{\n",
    "            \"pairs\": [\n",
    "                {{\n",
    "                    \"topic\": \"topic1\",\n",
    "                    \"details\": \"details1\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"topic\": \"topic2\",\n",
    "                    \"details\": \"details2\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Processing chunk {self.processed_count + 1}...\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": kv_cache_prompt.format(document=document)}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            seed=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        pairs = self._parse_into_pairs(response.choices[0].message.content)\n",
    "        if hasattr(self, 'json_parse_failures') and self._last_parse_used_fallback:\n",
    "            self.json_parse_failures += 1\n",
    "            if self.json_parse_failures > 0 and self.json_parse_failures % 5 == 0:\n",
    "               print(f\"Warning: JSON parsing has failed {self.json_parse_failures} times\")\n",
    "\n",
    "        print('*'*20)\n",
    "        print(f'first extracted pairs: {pairs[0:3]}\\n\\n')\n",
    "\n",
    "        # Batch process pairs\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "        for topic, details in pairs:\n",
    "            if not topic or not details:  # Skip empty pairs\n",
    "                continue\n",
    "\n",
    "            combined_text = f\"Topic: {topic}\\nDetails: {details}\"\n",
    "            texts.append(combined_text)\n",
    "            metadatas.append({\"topic\": topic})\n",
    "        print('*'*20)\n",
    "        print(f'num of extracted texts: {len(texts)}\\n\\n')\n",
    "        if self.store is None:\n",
    "            self.store = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
    "        else:\n",
    "            existing_docs = self.store.similarity_search(\"\")\n",
    "            existing_topics = {doc.metadata.get(\"topic\") for doc in existing_docs}\n",
    "\n",
    "            # Filter out duplicates\n",
    "            new_texts = []\n",
    "            new_metadatas = []\n",
    "            for text, metadata in zip(texts, metadatas):\n",
    "                if metadata[\"topic\"] not in existing_topics:\n",
    "                    new_texts.append(text)\n",
    "                    new_metadatas.append(metadata)\n",
    "\n",
    "            if new_texts:\n",
    "                self.store.add_texts(new_texts, metadatas=new_metadatas)\n",
    "\n",
    "        self.processed_count += 1\n",
    "        print(f\"Processed {self.processed_count} chunks so far\\n\\n\")\n",
    "\n",
    "    def _parse_into_pairs(self, content: str):\n",
    "        self._last_parse_used_fallback = False\n",
    "        try:\n",
    "            # Parse JSON and validate with Pydantic\n",
    "            data = json.loads(content)\n",
    "            validated_data = MemoryResponse(**data)\n",
    "            return [(pair.topic, pair.details) for pair in validated_data.pairs]\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            # Fall back to text parsing if JSON fails\n",
    "            self._last_parse_used_fallback = True\n",
    "            return self._parse_text_into_pairs(content)\n",
    "\n",
    "    def _parse_text_into_pairs(self, text: str):\n",
    "        \"\"\"Original text parsing method as fallback\"\"\"\n",
    "        pairs = []\n",
    "        lines = text.split('\\n')\n",
    "        current_topic = None\n",
    "        current_details = []\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith('Topic:'):\n",
    "                if current_topic:  # Save previous pair\n",
    "                    pairs.append((current_topic, ' '.join(current_details)))\n",
    "                current_topic = line[6:].strip()\n",
    "                current_details = []\n",
    "            elif line.startswith('Details:'):\n",
    "                current_details.append(line[8:].strip())\n",
    "            elif current_details:  # Continue adding to details if already started\n",
    "                current_details.append(line.strip())\n",
    "\n",
    "        # Add the last pair\n",
    "        if current_topic:\n",
    "            pairs.append((current_topic, ' '.join(current_details)))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def create_retrieval_queries(self, query: str) -> str | None:\n",
    "        \"\"\"Generate staging answers y = Θ_mem(q, D | θ_mem)\n",
    "        This should provide rough clues/outline to guide context retrieval\"\"\"\n",
    "\n",
    "        if not self.store:\n",
    "            return None\n",
    "\n",
    "        results = self.store.similarity_search_with_score(query, k=min(10, self.store.index.ntotal)) # consider increasing k to more than 10. maybe try even top 30% or something\n",
    "        relevant_info = [\n",
    "            f\"Topic: {doc.metadata['topic']}\\nDetails: {doc.page_content}\"\n",
    "            for doc, _ in results\n",
    "        ]\n",
    "\n",
    "        memorag_span_prompt = \"\"\"\n",
    "            You are given a question related to an article. To answer it effectively, you need to use specific details from the article. You are not provided with the whole article. Instead, you are provided with specific and relevant information from it.\n",
    "             Your task is to identify and extract one or more specific clue texts from the provided information that are relevant to the question.\n",
    "\n",
    "            ### Question: {question}\n",
    "            ### Information: {relevant_info}\n",
    "            ### Instructions:\n",
    "            1. You have a general understanding of the provided information. Your task is to generate one or more specific clues that will help in searching for supporting evidence within the article.\n",
    "            2. The clues are in the form of text spans that will assist in answering the question.\n",
    "            3. Only output the clues. If there are multiple clues, separate them with a newline.\n",
    "            \"\"\"\n",
    "        memorag_sur_prompt = \"\"\"\n",
    "            You are given a question related to an article. To answer it effectively, you need to use specific details from the article.  You are not provided with the whole article. Instead, you are provided with specific and relevant information from it.\n",
    "            Your task is to generate precise clue questions that can help locate the necessary information for answering the question.\n",
    "\n",
    "            ### Question: {question}\n",
    "            ### Information: {relevant_info}\n",
    "            ### Instructions:\n",
    "            1. You have a general understanding of the provided information. Your task is to generate one or more specific clues that will help in searching for supporting evidence within the article.\n",
    "            2. The clues are in the form of precise surrogate questions that clarify the original question.\n",
    "            3. Only output the clues. If there are multiple clues, separate them with a newline.\n",
    "            \"\"\"\n",
    "\n",
    "        system_prompt = \"You are an expert in information retrieval. Your task is to extract specific and relevant information from the provided context to help answer the given question.\"\n",
    "\n",
    "        text_spans = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": memorag_span_prompt.format(question=query, relevant_info = relevant_info)}\n",
    "            ]\n",
    "        ).choices[0].message.content\n",
    "        surrogate_queries = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": memorag_sur_prompt.format(question=query, relevant_info = relevant_info)}\n",
    "            ],\n",
    "            temperature=0.5  # Allow for some creativity in generating clues\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        retrieval_query = text_spans.split(\"\\n\") + surrogate_queries.split(\"\\n\")\n",
    "        retrieval_query = [q for q in retrieval_query if len(q.split()) > 3]\n",
    "        retrieval_query.append(query)\n",
    "\n",
    "        return retrieval_query\n",
    "\n",
    "    def save_store(self, path: str):\n",
    "        if self.store:\n",
    "            self.store.save_local(path)\n",
    "\n",
    "    def load_store(self, path: str):\n",
    "        self.store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)"
   ],
   "id": "5f823410ae7deb9b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Function",
   "id": "5129a38d93eef1ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:13.121137Z",
     "start_time": "2025-03-15T14:59:13.116785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_context(retrieval_query: str, vectorstore, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieve relevant context using the staging answer and the database vectorstore.\n",
    "    Implements c = Γ(y, D | γ) from the paper\"\"\"\n",
    "\n",
    "    results = vectorstore.similarity_search(retrieval_query, k=k)\n",
    "    contexts = [doc.page_content for doc in results]\n",
    "\n",
    "    return contexts"
   ],
   "id": "bc049fa0d88b8546",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generation Function",
   "id": "f38ef305cf0cc904"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:14.472876Z",
     "start_time": "2025-03-15T14:59:14.465163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_answer(query: str, contexts: List[str], temperature: float = 0) -> str:\n",
    "    \"\"\"Generate final answer y = Θ(q, c | θ)\"\"\"\n",
    "    prompt = f\"\"\"Based ONLY on the provided context, answer the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Context:\n",
    "{' '.join(contexts)}\n",
    "\n",
    "Provide a clear and concise answer based solely on the information in the context. If the context doesn't contain sufficient information to fully answer the query, clearly state that the necessary information is not available in the provided context rather than using any external knowledge.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in generating answers from given text. Provide clear, concise answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ],
   "id": "ab53db37ee3502ad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Query Processing Function",
   "id": "66ca6ece210c2c18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:15.894192Z",
     "start_time": "2025-03-15T14:59:15.890178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_query(query: str, memory_store, vectorstore):\n",
    "    print('*'*20)\n",
    "    print(\"\\nProcessing Query:\", query)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # y = Θ_mem(q, D | θ_mem)\n",
    "    retrieval_queries = memory_store.create_retrieval_queries(query)\n",
    "    print('*'*20)\n",
    "    print(f\"\\n\\nRetrieval Queries:\\n{retrieval_queries}\\n\\n\")\n",
    "\n",
    "    # c = Γ(y, D | γ)\n",
    "    all_contexts = []\n",
    "    for retrieval_query in retrieval_queries:\n",
    "        contexts = retrieve_context(retrieval_query, vectorstore, k=3)\n",
    "        all_contexts.extend(contexts)\n",
    "\n",
    "    unique_contexts = list(dict.fromkeys(all_contexts))  # Preserves order\n",
    "    # contexts = [retrieve_context(retrieval_query, vectorstore) for retrieval_query in retrieval_queries][0]\n",
    "    print('*'*20)\n",
    "    print(f\"Retrieved Context Example: {unique_contexts[0]}\\n\\n\")\n",
    "\n",
    "    # y = Θ(q, c | θ)\n",
    "    final_answer = generate_answer(query, unique_contexts)\n",
    "    print('*'*20)\n",
    "    print(f\"\\nFinal Answer: {final_answer}\")\n",
    "\n",
    "    return unique_contexts, final_answer"
   ],
   "id": "e3e4c2970f0f8641",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize Components",
   "id": "4f5e51437c7d37c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize memory store\n",
    "climate_memory_store = MemoryStore()\n",
    "\n",
    "# Load and process document\n",
    "path = \"../data/Understanding_Climate_Change.pdf\"\n",
    "# loader = PyPDFLoader(path)\n",
    "# documents = loader.load()\n",
    "# document_text = '\\n'.join([doc.page_content for doc in documents])\n",
    "# climate_memory_store.memorize(document_text)\n",
    "climate_vectorstore = encode_pdf(path, chunk_size=1000, chunk_overlap=200)"
   ],
   "id": "698e3786c420c76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# climate_memory_store.save_store(\"../data/Understanding_Climate_Change_Memory_Store.faiss\")",
   "id": "3ad4a718420f6a42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Usage Examples",
   "id": "63e823251233ca38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "climate_memory_store.load_store(\"../data/Understanding_Climate_Change_Memory_Store.faiss\")",
   "id": "fad841f534eaa11b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_1 = \"What are the impacts of climate change on biodiversity?\"\n",
    "query_2 = \"Please summarize the climate change article\"\n",
    "query_3 = \"Describe the social and economic influence of climate change.\"\n",
    "\n",
    "for query in [query_1, query_2, query_3]:\n",
    "    process_query(query, climate_memory_store, climate_vectorstore)\n"
   ],
   "id": "e458df9a279c725e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison Short Contex",
   "id": "13af1e5e102361c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple RAG",
   "id": "b9cbacfb48e89f72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:27.767055Z",
     "start_time": "2025-03-15T14:59:27.048411Z"
    }
   },
   "cell_type": "code",
   "source": "from evaluation.evalute_rag import *",
   "id": "8b7f2ce79457ce9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erantrabelci/workspace/RAG_Techniques/.venv/lib/python3.12/site-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.1.6, however version 2.5.5 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks_query_retriever = climate_vectorstore.as_retriever(search_kwargs={\"k\": 2})",
   "id": "afb3334c2386be5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_rag(chunks_query_retriever, q_a_file_name = \"../data/q_a.json\")",
   "id": "43ec0d785a1be500",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MemoRAG",
   "id": "8a5d25e5cdd41973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T15:00:22.089485Z",
     "start_time": "2025-03-15T15:00:22.084562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "\n",
    "def evaluate_memo_rag(memory_store = None,vectorstore = None, num_questions: int = 5,\n",
    "                      q_a_file_name = \"../data/q_a.json\",\n",
    "                      use_saved_results=True,\n",
    "                      results_path_prefix=\"../data/saved_evaluation_results.json\",\n",
    "                      use_original_memorag=False,\n",
    "                      original_memorag_answers=None,\n",
    "                      original_memorag_retrievals=None\n",
    "                      )-> None:\n",
    "    \"\"\"\n",
    "    Evaluate the MemoRAG system using predefined metrics.\n",
    "\n",
    "    Args:\n",
    "        memory_store: MemoryStore object containing the memory model.\n",
    "        vectorstore: FAISS vector store containing the encoded document chunks.\n",
    "        num_questions (int): Number of questions to evaluate (default: 5).\n",
    "        q_a_file_name (str): Path to the JSON file containing questions and answers (default: \"../data/q_a.json\").\n",
    "        use_saved_results (bool): Whether to load previously saved results or to save generated results to disk.\n",
    "        results_path_prefix (str): Path to save/load generated results.\n",
    "        use_original_memorag (bool): Whether to use paper's MemoRAG results or simplified MemoRAG implementation from this notebook.\n",
    "        original_memorag_answers: Pre-generated answers to use in case use_original_memorag=True.\n",
    "        original_memorag_retrievals: Pre-generated retrievals to use in case use_original_memorag=True.\n",
    "    \"\"\"\n",
    "    q_a_file_name = q_a_file_name\n",
    "    with open(q_a_file_name, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        q_a = json.load(json_file)\n",
    "\n",
    "    questions = [qa[\"question\"] for qa in q_a][:num_questions]\n",
    "    ground_truth_answers = [qa[\"answer\"] for qa in q_a][:num_questions]\n",
    "\n",
    "    full_results_path = f\"{results_path_prefix}_{q_a_file_name}\"\n",
    "    if use_saved_results and os.path.exists(full_results_path):\n",
    "        with open(full_results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            saved_data = json.load(f)\n",
    "            generated_answers = saved_data[\"generated_answers\"]\n",
    "            retrieved_documents = saved_data[\"retrieved_documents\"]\n",
    "        print(f\"Loaded previously saved results from {full_results_path}\")\n",
    "    else:\n",
    "        # Generate answers and retrieve documents for each question\n",
    "        generated_answers = []\n",
    "        retrieved_documents = []\n",
    "        for question in questions:\n",
    "            contexts, result = process_query(question, memory_store, vectorstore)\n",
    "            retrieved_documents.append(contexts)\n",
    "            generated_answers.append(result)\n",
    "\n",
    "        with open(full_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"generated_answers\": generated_answers,\n",
    "                \"retrieved_documents\": retrieved_documents\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved results to {full_results_path}\")\n",
    "\n",
    "    # Create test cases and evaluate\n",
    "    test_cases = create_deep_eval_test_cases(questions, ground_truth_answers, generated_answers, retrieved_documents)\n",
    "    # batch_size = 1\n",
    "\n",
    "    # for i in range(0, len(test_cases), batch_size):\n",
    "    #     batch = test_cases[i:i+batch_size]\n",
    "    #     evaluate(\n",
    "    #         test_cases=batch,\n",
    "    #         metrics=[correctness_metric, faithfulness_metric, relevance_metric],\n",
    "    #         throttle_value=5,\n",
    "    #         run_async = False,\n",
    "    #         ignore_errors = False,\n",
    "    #         print_results=False\n",
    "    #     )\n",
    "    #     time.sleep(15)\n",
    "\n",
    "    evaluate(\n",
    "        test_cases=test_cases,\n",
    "        metrics=[correctness_metric, faithfulness_metric, relevance_metric],\n",
    "        throttle_value = 5,\n",
    "        run_async = False,\n",
    "        ignore_errors = True\n",
    "    )"
   ],
   "id": "434e8409c789cc1a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_memo_rag(q_a_file_name = \"../data/q_a.json\", memory_store=climate_memory_store, vectorstore = climate_vectorstore)",
   "id": "4277785dedb39f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison Long Context",
   "id": "9bf1989911ebbe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:37.490574Z",
     "start_time": "2025-03-15T14:59:37.469436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize memory store\n",
    "wealth_memory_store = MemoryStore()\n",
    "\n",
    "# Load and process document\n",
    "path = \"../data/The_Wealth_of_Nations_Project_Gutenberg.pdf\"\n",
    "# loader = PyPDFLoader(path)\n",
    "# documents = loader.load()\n",
    "# document_text = '\\n'.join([doc.page_content for doc in documents])\n",
    "# Split document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50000,\n",
    "    chunk_overlap=5000\n",
    ")"
   ],
   "id": "d1dea5ea7a4fa7c7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# chunks = text_splitter.split_text(document_text)\n",
    "#\n",
    "# # Memorize each chunk\n",
    "# for chunk in chunks:\n",
    "#     wealth_memory_store.memorize(chunk)"
   ],
   "id": "308547a37c589f9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:59:46.401403Z",
     "start_time": "2025-03-15T14:59:46.268039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# wealth_memory_store.save_store(\"../data/The_Wealth_of_Nations_Project_Gutenberg_Memory_Store_Long_Context.faiss\")\n",
    "wealth_memory_store.load_store(\"../data/The_Wealth_of_Nations_Project_Gutenberg_Memory_Store_Long_Context.faiss\")"
   ],
   "id": "dc333fb67303673b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T15:00:22.069423Z",
     "start_time": "2025-03-15T14:59:50.794009Z"
    }
   },
   "cell_type": "code",
   "source": "wealth_vectorstore = encode_pdf(path, chunk_size=1000, chunk_overlap=200)",
   "id": "f38a66104097660e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple RAG",
   "id": "6cb501e70ae03669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks_query_retriever = wealth_vectorstore.as_retriever(search_kwargs={\"k\": 2})",
   "id": "26691a15572ee20b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_rag(chunks_query_retriever, num_questions = 15, q_a_file_name = \"../data/q_a_smith_short.json\")",
   "id": "dae32c71de154511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MemoRAG",
   "id": "cd0e0b63a097f5e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_memo_rag(num_questions=15, q_a_file_name = \"../data/q_a_smith_short.json\", memory_store= wealth_memory_store, vectorstore = wealth_vectorstore)",
   "id": "9baab1119bd1d994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate original MemoRAG",
   "id": "1c29f596081548a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "original_memorag_answers_wealth = [\"Adam Smith\",\n",
    "\"The invisible hand refers to the self-interested actions of individuals in a free market economy that unintentionally lead to economic outcomes.\",\n",
    "\"Land, labor, and capital.\",\n",
    "\"Labor determines the value of goods through the quantity of labor required to produce them.\",\n",
    "\"Adam Smith argues that while individuals act in their self-interest, this pursuit often benefits society more effectively than if they acted solely for personal gain. He illustrates this through examples like trade, where individuals acting in their self-interest actually facilitate economic growth and prosperity for society as a whole.\",\n",
    "\"Smith justifies taxation as necessary for the defense of the state, the maintenance of public order, and the provision of public services.\",\n",
    "\"Supply and demand determine prices. When supply exceeds demand, prices tend to fall; when demand exceeds supply, prices tend to rise.\",\n",
    "\"The division of labor encourages workers to focus on specific tasks, leading to innovation in tools and methods. This innovation enhances productivity and has long-term economic impacts by increasing the quantity of goods produced and the efficiency of production processes.\",\n",
    "\"Smith viewed agriculture as the primary source of wealth, while manufacturing contributed less significantly.\",\n",
    "\"Foreign trade enhances domestic wealth by allowing countries to specialize in producing goods where they have a comparative advantage. This specialization leads to increased efficiency and productivity. Smith's 'invisible hand' refers to the self-interested actions of individuals and businesses that, despite their self-interest, ultimately contribute to economic growth and efficiency.\",\n",
    "\"Capital accumulation increases the quantity of productive labor by enabling more workers to be employed. This leads to a division of labor where each worker specializes in a specific task, enhancing efficiency and productivity.\",\n",
    "\"Challenges include the need for security, the complexity of international transactions, and the potential for fraud and corruption.\",\n",
    "\"Value is the amount of labor required to produce a good or service.\",\n",
    "\"Smith believed that government should not interfere with the natural division of labor, allowing individuals to specialize in their areas of expertise. He argued that government's role is to protect property rights, ensure justice, and provide public works and institutions that facilitate economic growth.\",\n",
    "\"Free trade benefits consumers by lowering prices, increasing product variety, and enhancing economic efficiency.\",\n",
    "\"Adam Smith argued that monopolies, whether natural or artificial, are inherently detrimental to society and the economy. They stifle competition, lead to higher prices, and reduce overall economic efficiency. Smith believed that monopolies are a form of corruption and that the benefits they confer on a few at the expense of many are unjust. He advocated for free trade and competition as essential to economic prosperity and social justice\"\n",
    "]"
   ],
   "id": "42184b10cad36de7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "original_memorag_retrievals_wealth = [\n",
    "    [\n",
    "        \"The circumstances which seem to have introduced and established this policy are explained in the third book...\",\n",
    "        \"That which arises from the more solid improvements of agriculture is much more durable...\",\n",
    "        \"Among civilized and thriving nations, on the contrary, though a great number of people do not labour at all...\"\n",
    "    ],\n",
    "    [\n",
    "        \"Whenever he employs any part of it in maintaining unproductive hands of any kind...\",\n",
    "        \"The rich merchant, though with his capital he maintains industrious people only...\",\n",
    "        \"If there are any merchants among them, they are, properly, only the agents of wealthier merchants...\"\n",
    "    ],\n",
    "    [\n",
    "        \"As in a well ordered state of things, therefore, those ground expenses...\",\n",
    "        \"We should not call a marriage barren or unproductive, though it produced only a son and a daughter...\",\n",
    "        \"Among civilized and thriving nations, on the contrary, though a great number of people do not labour at all...\"\n",
    "    ],\n",
    "    [\n",
    "        \"The far greater part of them he must derive from the labour of other people...\",\n",
    "        \"His fortune is greater or less, precisely in proportion to the extent of this power...\",\n",
    "        \"A diamond, on the contrary, has scarce any value in use; but a very great quantity of other goods...\"\n",
    "    ],\n",
    "    [\n",
    "        \"Among competitors of equal wealth and luxury, the same deficiency will generally occasion...\",\n",
    "        \"If it is rent, the interest of the landlords will immediately prompt them...\",\n",
    "        \"The natural price, therefore, is, as it were, the central price, to which the prices of all commodities are continually gravitating...\"\n",
    "    ],\n",
    "    [\n",
    "        \"In every period, indeed, of every society, the surplus part both of the rude and manufactured produce...\",\n",
    "        \"It then became necessary to say something about the beneficial effects of foreign trade...\",\n",
    "        \"They naturally, perhaps necessarily, follow the mode of the times; and their expense comes to be regulated...\"\n",
    "    ],\n",
    "    [\n",
    "        \"The wages of the labour, and the profits of the stock employed in bringing such commodities to market...\",\n",
    "        \"First, by restraining the competition in some employments to a smaller number...\",\n",
    "        \"But the public would be a gainer, the work of all artificers coming in this way much cheaper to market...\"\n",
    "    ]\n",
    "]"
   ],
   "id": "79995ed6e1a48fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate_memo_rag(num_questions=15, q_a_file_name = \"../data/q_a_smith_short.json\", memory_store= wealth_memory_store, vectorstore = wealth_vectorstore, use_original_memorag=True, original_memorag_answers=original_memorag_answers_wealth, original_memorag_retrievals=original_memorag_retrievals_wealth)",
   "id": "11c43e62292ce78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import json\n",
    "#\n",
    "# def evaluate_original_memo_rag(num_questions: int = 15, q_a_file_name = \"../data/q_a.json\") -> None:\n",
    "#     \"\"\"\n",
    "#     Evaluate the MemoRAG system using predefined metrics.\n",
    "#\n",
    "#     Args:\n",
    "#         num_questions (int): Number of questions to evaluate (default: 5).\n",
    "#         q_a_file_name (str): Path to the JSON file containing questions and answers (default: \"../data/q_a.json\").\n",
    "#\n",
    "#     \"\"\"\n",
    "#     q_a_file_name = q_a_file_name\n",
    "#     with open(q_a_file_name, \"r\", encoding=\"utf-8\") as json_file:\n",
    "#         q_a = json.load(json_file)\n",
    "#\n",
    "#     questions = [qa[\"question\"] for qa in q_a][:num_questions]\n",
    "#     ground_truth_answers = [qa[\"answer\"] for qa in q_a][:num_questions]\n",
    "#     generated_answers = original_memorag_answers\n",
    "#     retrieved_documents = original_memorag_retrievals\n",
    "#\n",
    "#\n",
    "#     # Create test cases and evaluate\n",
    "#     test_cases = create_deep_eval_test_cases(questions, ground_truth_answers, generated_answers, retrieved_documents)\n",
    "#     evaluate(\n",
    "#         test_cases=test_cases,\n",
    "#         metrics=[correctness_metric, faithfulness_metric, relevance_metric],\n",
    "#         throttle_value = 3,\n",
    "#         run_async = True,\n",
    "#         ignore_errors = True\n",
    "#     )"
   ],
   "id": "ffeec86c0c18d006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # import time\n",
    "#\n",
    "# def evaluate_memo_rag(memory_store, vectorstore, num_questions: int = 5, q_a_file_name = \"../data/q_a.json\",\n",
    "#                     use_saved_results=True, results_path_prefix=\"../data/saved_evaluation_results.json\") -> None:\n",
    "#     \"\"\"\n",
    "#     Evaluate the MemoRAG system using predefined metrics.\n",
    "#\n",
    "#     Args:\n",
    "#         memory_store: MemoryStore object containing the memory model.\n",
    "#         vectorstore: FAISS vector store containing the encoded document chunks.\n",
    "#         num_questions (int): Number of questions to evaluate (default: 5).\n",
    "#         q_a_file_name (str): Path to the JSON file containing questions and answers (default: \"../data/q_a.json\").\n",
    "#         use_saved_results (bool): Whether to load previously saved results or to save generated results to disk.\n",
    "#         results_path_prefix (str): Path to save/load generated results.\n",
    "#\n",
    "#     \"\"\"\n",
    "#     q_a_file_name = q_a_file_name\n",
    "#     with open(q_a_file_name, \"r\", encoding=\"utf-8\") as json_file:\n",
    "#         q_a = json.load(json_file)\n",
    "#\n",
    "#     questions = [qa[\"question\"] for qa in q_a][:num_questions]\n",
    "#     ground_truth_answers = [qa[\"answer\"] for qa in q_a][:num_questions]\n",
    "#\n",
    "#     full_results_path = f\"{results_path_prefix}_{q_a_file_name}\"\n",
    "#     if use_saved_results and os.path.exists(full_results_path):\n",
    "#         with open(full_results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             saved_data = json.load(f)\n",
    "#             generated_answers = saved_data[\"generated_answers\"]\n",
    "#             retrieved_documents = saved_data[\"retrieved_documents\"]\n",
    "#         print(f\"Loaded previously saved results from {full_results_path}\")\n",
    "#     else:\n",
    "#         # Generate answers and retrieve documents for each question\n",
    "#         generated_answers = []\n",
    "#         retrieved_documents = []\n",
    "#         for question in questions:\n",
    "#             contexts, result = process_query(question, memory_store, vectorstore)\n",
    "#             retrieved_documents.append(contexts)\n",
    "#             generated_answers.append(result)\n",
    "#\n",
    "#         with open(full_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump({\n",
    "#                 \"generated_answers\": generated_answers,\n",
    "#                 \"retrieved_documents\": retrieved_documents\n",
    "#             }, f, ensure_ascii=False, indent=2)\n",
    "#         print(f\"Saved results to {full_results_path}\")\n",
    "#\n",
    "#     # Create test cases and evaluate\n",
    "#     test_cases = create_deep_eval_test_cases(questions, ground_truth_answers, generated_answers, retrieved_documents)\n",
    "#\n",
    "#     evaluate(\n",
    "#         test_cases=test_cases,\n",
    "#         metrics=[correctness_metric, faithfulness_metric, relevance_metric],\n",
    "#         throttle_value = 5,\n",
    "#         run_async = False,\n",
    "#         ignore_errors = True\n",
    "#     )"
   ],
   "id": "864e18fe2680ad86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# evaluate_original_memo_rag(num_questions=2, q_a_file_name = \"../data/q_a_smith_short_test.json\")",
   "id": "7c5a123041fbbd3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "90e8cb4eb3813afa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
