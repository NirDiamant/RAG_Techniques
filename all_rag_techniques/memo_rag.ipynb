{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MemoRAG: Enhancing Retrieval-Augmented Generation with Memory Models\n",
    "\n",
    "## Overview\n",
    "MemoRAG is a Retrieval-Augmented Generation (RAG) framework that incorporates a memory model as an auxiliary step before the retrieval phase. In doing so, it bridges the gap in contextual understanding and reasoning that standard RAG techniques face when addressing queries with implicit or ambiguous information needs and unstructured external knowledge.\n",
    "\n",
    "## Motivation\n",
    "Standard RAG techniques rely heavily on lexical or semantic matching between the query and the knowledge base. While this approach works well for clear question answering tasks with structured knowledge, it often falls short when handling queries with implicit or ambiguous information (e.g., describing the relationships between main characters in a novel) or when the knowledge base is unstructured (e.g., fiction books). In such cases, lexical or semantic matching seldom produces the desired outputs.\n",
    "\n",
    "## Key Components\n",
    "1. **Memory**: A compressed representation of the database created by a long-context model, designed to handle and summarize extensive inputs efficiently.\n",
    "2. **Retriever**: A standard RAG retrieval model responsible for selecting relevant context from the knowledge base to support the generator.\n",
    "3. **Generator**: A generative language model that produces responses by combining the query with the retrieved context, similar to standard RAG setups.\n",
    "\n",
    "## Method Details\n",
    "### 1. Memory\n",
    "- The memory module serves as an auxiliary component to enhance the retriever’s ability to identify better matches between queries and relevant parts of the database. It takes the original query and the database as inputs and produces staging answers — intermediate outputs like clues, surrogate queries, or key points — which the retriever uses instead of the original query.\n",
    "- Long-term memory is constructed by running a long-context model, such as Qwen2-7B-Instruct or Mistral-7B-Instruct-v0.2, over the entire database. This process generates a compressed representation of the database through an attention mechanism.\n",
    "- The compressed representation is stored as key-value pairs, facilitating efficient and accurate retrieval.\n",
    "- Released memory models include memorag-qwen2-7b-inst and memorag-mistral-7b-inst, derived from Qwen2-7B-Instruct and Mistral-7B-Instruct-v0.2, respectively.\n",
    "\n",
    "### 2. Retriever\n",
    "- The retriever is a standard retrieval model, adapted to take processed queries (created by the memory module as staging answers) instead of the original query.\n",
    "- It outputs the retrieved **context**, which serves as the basis for generating the final answer.\n",
    "\n",
    "\n",
    "### 3. Generator\n",
    "- The generator produces the final response by combining the retriever’s output (retrieved context) with the original query.\n",
    "- MemoRAG ensures compatibility and consistency by using the memory module’s underlying model as the default generator.\n",
    "\n",
    "## Benefits of the Approach\n",
    "1. **Extended Scope of Queries:** MemoRAG's preprocessing capabilities enable it to handle complex and long-context tasks that conventional RAG methods struggle with.\n",
    "\n",
    "2. **Improved Accuracy:** By simplifying and adjusting queries before retrieval, MemoRAG enhances performance over standard RAG methods.\n",
    "\n",
    "3. **Flexibility:** Adapts to diverse tasks, datasets, and retrieval scenarios.\n",
    "\n",
    "4. **Robustness:** Improved performance remains consistent across various generators, datasets, and query types.\n",
    "\n",
    "5. **Efficiency**: The use of key-value compression reduces computational overhead.\n",
    "\n",
    "## Conclusion\n",
    "The memory module in MemoRAG significantly enhances comprehension of both the queries and the database, enabling more effective retrieval. Its ability to preprocess queries, generate staging answers, and leverage long-context memory models ensures high-quality responses, making MemoRAG a significant step forward in the evolution of retrieval-augmented generation.\n"
   ],
   "id": "67e98dec56de7fdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/memo_rag.svg\" alt=\"MemoRAG\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ],
   "id": "abfa998b6aa7d7ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "2bc3b3e34c811bd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:09.212048Z",
     "start_time": "2025-01-14T23:37:08.713603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from openai import OpenAI\n",
    "import time"
   ],
   "id": "e31a95191a274c83",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAI Setup",
   "id": "a84d4f5a1fd94512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:09.237602Z",
     "start_time": "2025-01-14T23:37:09.227443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ],
   "id": "705080e6703ad71c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory Base Classes",
   "id": "62fd38f51251d71a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:09.338396Z",
     "start_time": "2025-01-14T23:37:09.334982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleMemoryStore:\n",
    "    \"\"\"Simple memory store using FAISS\"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.episodic_store = None\n",
    "        self.semantic_store = None\n",
    "\n",
    "    def add_memory(self, text: str, memory_type: str):\n",
    "        \"\"\"Add a memory to either episodic or semantic store\"\"\"\n",
    "        if memory_type == \"episodic\":\n",
    "            if self.episodic_store is None:\n",
    "                self.episodic_store = FAISS.from_texts([text], self.embeddings)\n",
    "            else:\n",
    "                self.episodic_store.add_texts([text])\n",
    "        elif memory_type == \"semantic\":\n",
    "            if self.semantic_store is None:\n",
    "                self.semantic_store = FAISS.from_texts([text], self.embeddings)\n",
    "            else:\n",
    "                self.semantic_store.add_texts([text])\n",
    "\n",
    "    def search_memories(self, query: str, k: int = 2) -> Dict[str, List[tuple[str, float]]]:\n",
    "        \"\"\"Search both memory stores\"\"\"\n",
    "        results = {\"episodic\": [], \"semantic\": []}\n",
    "\n",
    "        if self.episodic_store:\n",
    "            episodic_results = self.episodic_store.similarity_search_with_score(query, k=k)\n",
    "            results[\"episodic\"] = [(doc.page_content, score) for doc, score in episodic_results]\n",
    "\n",
    "        if self.semantic_store:\n",
    "            semantic_results = self.semantic_store.similarity_search_with_score(query, k=k)\n",
    "            results[\"semantic\"] = [(doc.page_content, score) for doc, score in semantic_results]\n",
    "\n",
    "        return results"
   ],
   "id": "5f823410ae7deb9b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize Components",
   "id": "aaf784ac3a6742c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:12.183609Z",
     "start_time": "2025-01-14T23:37:09.341611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize memory store\n",
    "memory_store = SimpleMemoryStore()\n",
    "\n",
    "# Load and process document\n",
    "path = \"../data/Understanding_Climate_Change.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "vectorstore = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ],
   "id": "cb33919c6efcc3f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/mnpdpbvj4w955kx4v0v62b880000gn/T/ipykernel_35835/1861107860.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  self.embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example Memory Creation",
   "id": "37af854bca833bc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:13.907242Z",
     "start_time": "2025-01-14T23:37:12.188383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add example memories\n",
    "memory_store.add_memory(\n",
    "    \"The impact of rising temperatures on coral reefs is devastating, leading to widespread bleaching.\",\n",
    "    \"episodic\"\n",
    ")\n",
    "\n",
    "memory_store.add_memory(\n",
    "    \"Greenhouse gases trap heat in Earth's atmosphere, leading to global warming.\",\n",
    "    \"semantic\"\n",
    ")"
   ],
   "id": "343afdd5ea31e37f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search and Generation Functions",
   "id": "5129a38d93eef1ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:13.930Z",
     "start_time": "2025-01-14T23:37:13.921548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def enhanced_search(query: str, memory_store, vectorstore, max_time: int = 60):\n",
    "    \"\"\"Perform enhanced search with memory integration\"\"\"\n",
    "    print(\"\\nStarting enhanced search...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Get relevant memories\n",
    "        memories = memory_store.search_memories(query, k=2)\n",
    "        print(\"\\nRelevant memories found:\")\n",
    "        for mem_type, mem_list in memories.items():\n",
    "            for content, score in mem_list:\n",
    "                relevance = 1 / (1 + score)\n",
    "                print(f\"- {mem_type.title()} Memory (relevance: {relevance:.2f}):\")\n",
    "                print(f\"  {content[:100]}...\")\n",
    "\n",
    "        # Combine query with memories for better search\n",
    "        memory_context = \" \".join([mem[0] for mems in memories.values() for mem in mems])\n",
    "        enhanced_query = f\"{query} Context: {memory_context}\"\n",
    "\n",
    "        # Search document\n",
    "        results = vectorstore.similarity_search(enhanced_query, k=2)\n",
    "        contexts = [doc.page_content for doc in results]\n",
    "\n",
    "        print(f\"\\nSearch completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        if time.time() - start_time > max_time:\n",
    "            return [], memories\n",
    "\n",
    "        return contexts, memories\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return [], {}\n",
    "\n",
    "def generate_answer(query: str, contexts: List[str], memories: Dict[str, List[tuple[str, float]]]) -> str:\n",
    "    \"\"\"Generate concise answer using contexts and memories\"\"\"\n",
    "    memory_text = \"\\n\".join([\n",
    "        f\"{mem_type.title()} Memory: {content}\"\n",
    "        for mem_type, mem_list in memories.items()\n",
    "        for content, _ in mem_list\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Provide a concise answer to the query using the following information.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Context:\n",
    "{' '.join(contexts)}\n",
    "\n",
    "Related Memories:\n",
    "{memory_text}\n",
    "\n",
    "Focus on key impacts and implications. Be clear and concise.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant. Provide clear, concise answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ],
   "id": "bc049fa0d88b8546",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Usage Example",
   "id": "63e823251233ca38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:37:18.212358Z",
     "start_time": "2025-01-14T23:37:13.936264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example query\n",
    "print(\"\\nProcessing Query:\", \"What are the impacts of climate change on biodiversity?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "contexts, memories = enhanced_search(query, memory_store, vectorstore)\n",
    "\n",
    "if contexts:\n",
    "    print(\"\\nRetrieved Contexts:\")\n",
    "    for i, context in enumerate(contexts, 1):\n",
    "        print(f\"\\nContext {i}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(context[:200] + \"...\" if len(context) > 200 else context)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    answer = generate_answer(query, contexts, memories)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(answer)\n",
    "    print(\"-\" * 50)"
   ],
   "id": "b00be38f336c4d7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Query: What are the impacts of climate change on biodiversity?\n",
      "==================================================\n",
      "\n",
      "Starting enhanced search...\n",
      "\n",
      "Relevant memories found:\n",
      "- Episodic Memory (relevance: 0.75):\n",
      "  The impact of rising temperatures on coral reefs is devastating, leading to widespread bleaching....\n",
      "- Semantic Memory (relevance: 0.71):\n",
      "  Greenhouse gases trap heat in Earth's atmosphere, leading to global warming....\n",
      "\n",
      "Search completed in 2.81 seconds\n",
      "\n",
      "Retrieved Contexts:\n",
      "\n",
      "Context 1:\n",
      "==================================================\n",
      "The Arctic is warming at more than twice the global average rate, leading to significant ice \n",
      "loss. Antarctic ice sheets are also losing mass, contributing to sea level rise. This melting \n",
      "affects glo...\n",
      "--------------------------------------------------\n",
      "\n",
      "Context 2:\n",
      "==================================================\n",
      "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
      "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
      "fisheries. Pr...\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Answer:\n",
      "==================================================\n",
      "Climate change impacts biodiversity by causing glacial retreat, coastal erosion, extreme weather events, hurricanes and typhoons, droughts, flooding, ocean acidification, coral reef bleaching, and disruption of marine ecosystems. This threatens ecosystems, species survival, fisheries, and human livelihoods. Mitigation through renewable energy and adaptation strategies is crucial to minimize these impacts.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
