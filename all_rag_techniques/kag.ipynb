{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAG: Knowledge Augmented Generation\n",
    "\n",
    " \n",
    "## Overview\n",
    "\n",
    " \n",
    "KAG is a logical reasoning and Q&A framework based on the OpenSPG engine and large language models, which is used to build logical reasoning and Q&A solutions for vertical domain knowledge bases. KAG can effectively overcome the ambiguity of traditional RAG vector similarity calculation and the noise problem of GraphRAG introduced by OpenIE. KAG supports logical reasoning and multi-hop fact Q&A, etc., and is significantly better than the current SOTA method\n",
    "\n",
    "## Motivation\n",
    "\n",
    " \n",
    "The goal of KAG is to build a knowledge-enhanced LLM service framework in professional domains, supporting logical reasoning, factual Q&A, etc. KAG fully integrates the logical and factual characteristics of the KGs. Its core features include:\n",
    "Knowledge and Chunk Mutual Indexing structure to integrate more complete contextual text information.\n",
    "Knowledge alignment using conceptual semantic reasoning to alleviate the noise problem caused by OpenIE.\n",
    "Schema-constrained knowledge construction to support the representation and construction of domain expert knowledge.\n",
    "Logical form-guided hybrid reasoning and retrieval to support logical reasoning and multi-hop reasoning Q&A.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "Knowledge Graph Generation: Constructs graphs with entities as nodes and relationships as edges.\n",
    "Community Detection: Identifies clusters of related entities within the graph.\n",
    "Summarization: Generates summaries for each community to provide context for LLMs.\n",
    "Query Processing: Uses these summaries to enhance the LLM's ability to answer complex questions.\n",
    "## Method Details\n",
    "\n",
    "Indexing Stage\n",
    "\n",
    " \n",
    "Text Chunking: Splits source texts into manageable chunks.\n",
    "Element Extraction: Uses LLMs to identify entities and relationships.\n",
    "Graph Construction: Builds a graph from the extracted elements.\n",
    "Community Detection: Applies algorithms like Leiden to find communities.\n",
    "Community Summarization: Creates summaries for each community.\n",
    "\n",
    "Query Stage\n",
    "\n",
    " \n",
    "Knowledge Representation.\n",
    "Mixed Reasoning Guided by Logic Forms.\n",
    "\n",
    "\n",
    "KAG ( [openspg.github.io](https://openspg.github.io/v2/) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/Microsoft_GraphRag.svg\" alt=\"adaptive retrieval\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you can use either OpenAI API key or Azure OpenAI key. \n",
    "Create a `.env` file and fill in the credentials for your OpenAI or Azure Open AI deployment. The following code loads these environment variables and sets up our AI client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "GPT4O_MODEL_NAME=\"gpt-4o\"\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT_NAME=\"\"\n",
    "AZURE_OPENAI_API_VERSION=\"2024-06-01\"\n",
    "\n",
    "OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install beautifulsoup4 openai python-dotenv pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook. If you're running this notebook in a new environment, execute this cell first to ensure all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "\n",
    "AZURE=True #Change to False to use OpenAI\n",
    "if AZURE:\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    GPT4O_DEPLOYMENT_NAME = os.getenv(\"GPT4O_MODEL_NAME\")\n",
    "    TEXT_EMBEDDING_3_LARGE_NAME = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT_NAME\")\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    oai = AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, api_key=AZURE_OPENAI_API_KEY, api_version=AZURE_OPENAI_API_VERSION)\n",
    "else:\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    oai = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by getting a text to work with. The Wikipedia article on Elon Musk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Elon_Musk\"  # Replace with the URL of the web page you want to scrape\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "if not os.path.exists('data'): \n",
    "    os.makedirs('data')\n",
    "\n",
    "if not os.path.exists('data/elon.md'):\n",
    "    elon = soup.text.split('\\nSee also')[0]\n",
    "    with open('data/elon.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(elon)\n",
    "else:\n",
    "    with open('data/elon.md', 'r') as f:\n",
    "        elon = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphRag has a convenient set of CLI commands we can use. We'll start by configuring the system, then run the indexing operation. Indexing with GraphRag is a much lengthier process, and one that costs significantly more, since rather than just calculating embeddings, GraphRag makes many LLM calls to analyse the text, extract entities, and construct the graph. That's a one-time expense, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if not os.path.exists('data/graphrag'):\n",
    "    !python -m graphrag.index --init --root data/graphrag\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'r') as f:\n",
    "    settings_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "settings_yaml['llm']['model'] = \"gpt-4o\"\n",
    "settings_yaml['llm']['api_key'] = AZURE_OPENAI_API_KEY if AZURE else OPENAI_API_KEY\n",
    "settings_yaml['llm']['type'] = 'azure_openai_chat' if AZURE else 'openai_chat'\n",
    "settings_yaml['embeddings']['llm']['api_key'] = AZURE_OPENAI_API_KEY if AZURE else OPENAI_API_KEY\n",
    "settings_yaml['embeddings']['llm']['type'] = 'azure_openai_embedding' if AZURE else 'openai_embedding'\n",
    "settings_yaml['embeddings']['llm']['model'] = TEXT_EMBEDDING_3_LARGE_NAME if AZURE else 'text-embedding-3-large'\n",
    "if AZURE:\n",
    "    settings_yaml['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "    settings_yaml['llm']['deployment_name'] = GPT4O_DEPLOYMENT_NAME\n",
    "    settings_yaml['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "    settings_yaml['embeddings']['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "    settings_yaml['embeddings']['llm']['deployment_name'] = TEXT_EMBEDDING_3_LARGE_NAME\n",
    "    settings_yaml['embeddings']['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'w') as f:\n",
    "    yaml.dump(settings_yaml, f)\n",
    "\n",
    "if not os.path.exists('data/graphrag/input'):\n",
    "    os.makedirs('data/graphrag/input')\n",
    "    !cp data/elon.md data/graphrag/input/elon.txt\n",
    "    !python -m graphrag.index --root ./data/graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an output:\n",
    "\ud83d\ude80 \u001bAll workflows completed successfully.\u001b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query GraphRag we'll use its CLI again, making sure to configure it with a context length equivalent to what we use in our embeddings search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "DEFAULT_RESPONSE_TYPE = 'Summarize and explain in 1-2 paragraphs with bullet points using at most 300 tokens'\n",
    "DEFAULT_MAX_CONTEXT_TOKENS = 10000\n",
    "\n",
    "def remove_data(text):\n",
    "    return re.sub(r'\\[Data:.*?\\]', '', text).strip()\n",
    "\n",
    "\n",
    "def ask_graph(query,method):\n",
    "    env = os.environ.copy() | {\n",
    "      'GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS': str(DEFAULT_MAX_CONTEXT_TOKENS),\n",
    "    }\n",
    "    command = [\n",
    "      'python', '-m', 'graphrag.query',\n",
    "      '--root', './data/graphrag',\n",
    "      '--method', method,\n",
    "      '--response_type', DEFAULT_RESPONSE_TYPE,\n",
    "      query,\n",
    "    ]\n",
    "    output = subprocess.check_output(command, universal_newlines=True, env=env, stderr=subprocess.DEVNULL)\n",
    "    return remove_data(output.split('Search Response: ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GrpahRag offers 2 types of search:\n",
    "1. Global Search for reasoning about holistic questions about the corpus by leveraging the community summaries.\n",
    "2. Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the local search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Elon Musk has founded several companies and subsidiaries across various industries. Here's a summary:\n",
       "\n",
       "- **SpaceX**: Founded in 2002, SpaceX is a private aerospace manufacturer and space transportation company. Musk serves as the CEO and chief engineer .\n",
       "\n",
       "- **Tesla, Inc.**: Although not originally founded by Musk, he became an early investor and later the CEO and product architect, significantly shaping its direction .\n",
       "\n",
       "- **Neuralink**: Co-founded by Musk, this company focuses on developing brain-machine interfaces to enhance human-computer interaction .\n",
       "\n",
       "- **The Boring Company**: Founded by Musk, it specializes in tunnel construction and innovative transportation solutions .\n",
       "\n",
       "- **X.com/PayPal**: Musk co-founded X.com, which later became PayPal after merging with Confinity .\n",
       "\n",
       "- **Zip2**: Co-founded with his brother Kimbal, this was Musk's first venture, later acquired by Compaq .\n",
       "\n",
       "- **SolarCity**: Co-created by Musk, it was later acquired by Tesla and rebranded as Tesla Energy .\n",
       "\n",
       "- **xAI**: Founded in 2023, this company focuses on artificial intelligence research .\n",
       "\n",
       "- **OpenAI**: Co-founded by Musk, this nonprofit organization is dedicated to AI research .\n",
       "\n",
       "In total, Musk has founded or co-founded at least nine companies and subsidiaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "local_query=\"What and how many companies and subsidieries founded by Elon Musk\"\n",
    "local_result = ask_graph(local_query,'local')\n",
    "\n",
    "Markdown(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Elon Musk has achieved significant accomplishments across various industries, demonstrating his influence and innovation:\n",
       "\n",
       "- **Space Exploration**: Founder, CEO, and chief engineer of SpaceX, Musk has propelled the company to the forefront of space exploration and satellite deployment, establishing it as a leading spaceflight services provider .\n",
       "\n",
       "- **Automotive Industry**: As CEO of Tesla, Musk has driven the company to the forefront of electric vehicles and sustainable energy, significantly impacting the automotive industry with innovations in electric cars and energy solutions .\n",
       "\n",
       "- **Online Payments**: Co-founded X.com, which evolved into PayPal, revolutionizing online transactions and becoming a major player in the online payment industry .\n",
       "\n",
       "- **Neural Technology**: Co-founded Neuralink, focusing on advancing brain-machine interface technology to enhance the connection between the human brain and computers .\n",
       "\n",
       "- **Infrastructure**: Founded The Boring Company, specializing in tunnel construction to reduce traffic congestion through innovative underground transportation systems ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_query=\"What are the major accomplishments of Elon Musk?\"\n",
    "global_result = ask_graph(global_query,'global')\n",
    "\n",
    "Markdown(global_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
